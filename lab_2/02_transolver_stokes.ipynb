{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Training Transolver on Stokes Flow\n",
        "\n",
        "**Goal:** Train a Transolver model and visualize how it learns to partition the mesh into physics-based slices.\n",
        "\n",
        "## Outline\n",
        "1. Load Stokes Flow Dataset\n",
        "2. Transolver Model (PhysicsNeMo)\n",
        "3. Training Loop (200 Epochs)\n",
        "4. Visualize Learned Slice Assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from utils import download_stokes_dataset, load_stokes_sample, get_num_samples\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Stokes Flow Dataset\n",
        "\n",
        "We'll use the same dataset from Lab 4: Stokes flow around obstacles with varying geometries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and load dataset\n",
        "download_stokes_dataset()\n",
        "\n",
        "# Load multiple samples for training\n",
        "num_samples = max(1, min(get_num_samples(), 50))  # Use up to 50 samples\n",
        "print(f\"Loading {num_samples} samples...\")\n",
        "\n",
        "train_data = []\n",
        "for i in range(num_samples):\n",
        "    coords, u, v, p = load_stokes_sample(sample_idx=i)\n",
        "    train_data.append({\n",
        "        'coords': torch.tensor(coords, dtype=torch.float32),\n",
        "        'targets': torch.tensor(np.stack([u, v, p], axis=1), dtype=torch.float32)\n",
        "    })\n",
        "\n",
        "# Use first sample for visualization\n",
        "sample_coords = train_data[0]['coords']\n",
        "sample_targets = train_data[0]['targets']\n",
        "N = len(sample_coords)\n",
        "print(f\"✓ Loaded {num_samples} samples, each with ~{N} mesh points\")\n",
        "print(f\"  Input: coordinates (N, 2)\")\n",
        "print(f\"  Output: u, v, p (N, 3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Transolver Model\n",
        "\n",
        "We'll try to use PhysicsNeMo's Transolver if available, otherwise fall back to a simple implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import PhysicsNeMo Transolver\n",
        "from physicsnemo.models.transolver import Transolver\n",
        "\n",
        "# Model configuration\n",
        "NUM_SLICES = 32  # Number of learned slices\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 4\n",
        "NUM_HEADS = 8\n",
        "\n",
        "print(\"✓ PhysicsNeMo Transolver imported\")\n",
        "print(f\"  Config: hidden_dim={HIDDEN_DIM}, layers={NUM_LAYERS}, slices={NUM_SLICES}, heads={NUM_HEADS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create PhysicsNeMo Transolver model\n",
        "# For unstructured meshes: embedding_dim=2 (x,y coordinates), functional_dim=0 (no additional input features)\n",
        "model = Transolver(\n",
        "    functional_dim=0,           # No functional input, just coordinates\n",
        "    out_dim=3,                  # Output: u, v, p\n",
        "    embedding_dim=2,            # 2D coordinates as embeddings\n",
        "    n_layers=NUM_LAYERS,\n",
        "    n_hidden=HIDDEN_DIM,\n",
        "    n_head=NUM_HEADS,\n",
        "    slice_num=NUM_SLICES,\n",
        "    unified_pos=False,          # We provide our own embeddings (coordinates)\n",
        "    structured_shape=None,      # Irregular/unstructured mesh\n",
        "    use_te=False,               # Don't require transformer engine\n",
        ").to(device)\n",
        "\n",
        "# Storage for captured slice weights\n",
        "captured_slice_weights = {}\n",
        "\n",
        "def capture_slice_weights_hook(module, input, output):\n",
        "    \"\"\"Hook to capture slice weights from PhysicsAttention forward pass.\"\"\"\n",
        "    # The slice weights are computed in compute_slices_from_projections\n",
        "    # We need to access them through the module's internal state\n",
        "    # After forward, we can recompute them from the stored projections\n",
        "    pass\n",
        "\n",
        "# Helper function to get slice weights by running a modified forward pass\n",
        "def get_slice_weights(model, coords):\n",
        "    \"\"\"\n",
        "    Extract slice weights from the first PhysicsAttention layer.\n",
        "    \n",
        "    PhysicsNeMo's Transolver computes slice_weights internally in each block's Attn module.\n",
        "    We access the first block and manually compute slice weights from the projections.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Prepare input (PhysicsNeMo expects: fx=functional_input, embedding=spatial_coords)\n",
        "        B, N, _ = coords.shape\n",
        "        fx = torch.zeros(B, N, 0, device=coords.device)  # Empty functional input\n",
        "        embedding = coords  # Coordinates as embeddings\n",
        "        \n",
        "        # Run through preprocessing\n",
        "        fx_combined = torch.cat((embedding, fx), -1)  # (B, N, 2)\n",
        "        h = model.preprocess(fx_combined)  # (B, N, hidden_dim)\n",
        "        \n",
        "        # Get first block's attention module\n",
        "        first_block = model.blocks[0]\n",
        "        attn_module = first_block.Attn\n",
        "        \n",
        "        # Run through LayerNorm\n",
        "        h_normed = first_block.ln_1(h)\n",
        "        \n",
        "        # Project to head dimension (PhysicsNeMo internal)\n",
        "        x_mid = attn_module.in_project_x(h_normed)\n",
        "        x_mid = x_mid.view(B, N, attn_module.heads, attn_module.dim_head)\n",
        "        \n",
        "        # Compute slice projections\n",
        "        slice_projections = attn_module.in_project_slice(x_mid)  # (B, N, heads, slices)\n",
        "        \n",
        "        # Compute slice weights (temperature-scaled softmax)\n",
        "        temp = torch.clamp(attn_module.temperature, min=0.5, max=5)\n",
        "        slice_weights = F.softmax(slice_projections / temp, dim=-1)  # (B, N, heads, slices)\n",
        "        \n",
        "        # Average across heads for visualization\n",
        "        slice_weights_avg = slice_weights.mean(dim=2)  # (B, N, slices)\n",
        "        \n",
        "        return slice_weights_avg\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✓ PhysicsNeMo Transolver created: {n_params:,} parameters\")\n",
        "print(f\"  Architecture: {NUM_LAYERS} layers × {NUM_HEADS} heads × {NUM_SLICES} slices\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train for 20 Epochs\n",
        "\n",
        "We'll train the model and track how the slice assignments evolve during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "losses = []\n",
        "slice_history = []  # Store slice assignments at different epochs\n",
        "\n",
        "# Get initial slice assignments (before training)\n",
        "x = sample_coords.unsqueeze(0).to(device)\n",
        "initial_slices = get_slice_weights(model, x)[0].cpu().numpy()\n",
        "slice_history.append(('Epoch 0 (untrained)', initial_slices))\n",
        "\n",
        "print(\"Training PhysicsNeMo Transolver...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    # Shuffle data each epoch\n",
        "    indices = np.random.permutation(len(train_data))\n",
        "    \n",
        "    for idx in indices:\n",
        "        data = train_data[idx]\n",
        "        coords = data['coords'].unsqueeze(0).to(device)  # (1, N, 2) - used as embedding\n",
        "        targets = data['targets'].unsqueeze(0).to(device)  # (1, N, 3)\n",
        "        \n",
        "        # PhysicsNeMo Transolver forward: fx=None (no functional input), embedding=coords\n",
        "        optimizer.zero_grad()\n",
        "        B, N, _ = coords.shape\n",
        "        fx = torch.zeros(B, N, 0, device=device)  # Empty functional input\n",
        "        pred = model(fx, embedding=coords)\n",
        "        loss = criterion(pred, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_data)\n",
        "    losses.append(avg_loss)\n",
        "    \n",
        "    # Save slice assignments at key epochs\n",
        "    if (epoch + 1) in [10, 25, 50, 100]:\n",
        "        x = sample_coords.unsqueeze(0).to(device)\n",
        "        slices = get_slice_weights(model, x)[0].cpu().numpy()\n",
        "        slice_history.append((f'Epoch {epoch+1}', slices))\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS}: Loss = {avg_loss:.6f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"✓ Training complete! Final loss: {losses[-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Key Takeaways\n",
        "\n",
        "Let's see how the model learned to partition the mesh during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, NUM_EPOCHS+1), losses, 'b-o', linewidth=2, markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.semilogy(range(1, NUM_EPOCHS+1), losses, 'b-o', linewidth=2, markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss (log scale)')\n",
        "plt.title('Training Loss (Log Scale)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how slice assignments evolved during training\n",
        "coords_np = sample_coords.numpy()\n",
        "num_snapshots = len(slice_history)\n",
        "\n",
        "fig, axes = plt.subplots(1, num_snapshots, figsize=(4*num_snapshots, 4))\n",
        "\n",
        "for idx, (title, slice_weights) in enumerate(slice_history):\n",
        "    ax = axes[idx] if num_snapshots > 1 else axes\n",
        "    \n",
        "    # Get dominant slice for each point\n",
        "    dominant_slice = np.argmax(slice_weights, axis=1)\n",
        "    \n",
        "    # Plot mesh colored by slice\n",
        "    scatter = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=dominant_slice, \n",
        "                        cmap='tab10', s=6, alpha=0.7)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "plt.suptitle('Evolution of Learned Slice Assignments During Training', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how slices become more spatially coherent as training progresses!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comparison: Learned slices vs actual physics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Get final slice assignments\n",
        "final_slices = slice_history[-1][1]\n",
        "dominant_slice = np.argmax(final_slices, axis=1)\n",
        "targets_np = sample_targets.numpy()\n",
        "\n",
        "# Top left: Velocity u\n",
        "ax = axes[0, 0]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=targets_np[:, 0], cmap='RdBu_r', s=6)\n",
        "ax.set_title('Ground Truth: Velocity u', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "\n",
        "# Top right: Velocity v\n",
        "ax = axes[0, 1]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=targets_np[:, 1], cmap='RdBu_r', s=6)\n",
        "ax.set_title('Ground Truth: Velocity v', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "\n",
        "# Bottom left: Pressure\n",
        "ax = axes[1, 0]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=targets_np[:, 2], cmap='viridis', s=6)\n",
        "ax.set_title('Ground Truth: Pressure p', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "\n",
        "# Bottom right: Learned slices\n",
        "ax = axes[1, 1]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=dominant_slice, cmap='tab10', s=6, alpha=0.8)\n",
        "ax.set_title(f'Learned Slice Assignments (M={NUM_SLICES})', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "cbar = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "cbar.set_label('Slice ID')\n",
        "\n",
        "plt.suptitle('Physics Fields vs Learned Slice Partitioning', fontsize=13, y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The model learns to group points with similar physical behavior into slices!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**What we demonstrated:**\n",
        "1. Trained a Transolver model on Stokes flow data for 20 epochs\n",
        "2. Visualized how **slice assignments evolve** during training\n",
        "3. The model learns to partition the mesh based on **physical behavior**, not just geometry\n",
        "\n",
        "**Key observations:**\n",
        "- Initially (untrained): slices are essentially random\n",
        "- After training: slices align with physics (e.g., inlet, wake, boundaries)\n",
        "- This learned partitioning enables efficient O(N·M) attention instead of O(N²)\n",
        "**References:**\n",
        "- [Transolver Paper](https://arxiv.org/abs/2402.02366)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
