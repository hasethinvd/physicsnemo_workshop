{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Training Transolver on Stokes Flow\n",
        "\n",
        "**Goal:** Train a Transolver model and visualize how it learns to partition the mesh into physics-based slices.\n",
        "\n",
        "## Outline\n",
        "1. Load Stokes Flow Dataset\n",
        "2. Transolver Model Implementation\n",
        "3. Train for 20 Epochs\n",
        "4. Visualize Learned Slice Assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from utils import download_stokes_dataset, load_stokes_sample, get_num_samples\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Stokes Flow Dataset\n",
        "\n",
        "We'll use the same dataset from Lab 2: Stokes flow around obstacles with varying geometries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download and load dataset\n",
        "download_stokes_dataset()\n",
        "\n",
        "# Load multiple samples for training\n",
        "num_samples = max(1, min(get_num_samples(), 50))  # Use up to 50 samples\n",
        "print(f\"Loading {num_samples} samples...\")\n",
        "\n",
        "train_data = []\n",
        "for i in range(num_samples):\n",
        "    coords, u, v, p = load_stokes_sample(sample_idx=i)\n",
        "    train_data.append({\n",
        "        'coords': torch.tensor(coords, dtype=torch.float32),\n",
        "        'targets': torch.tensor(np.stack([u, v, p], axis=1), dtype=torch.float32)\n",
        "    })\n",
        "\n",
        "# Use first sample for visualization\n",
        "sample_coords = train_data[0]['coords']\n",
        "sample_targets = train_data[0]['targets']\n",
        "N = len(sample_coords)\n",
        "print(f\"✓ Loaded {num_samples} samples, each with ~{N} mesh points\")\n",
        "print(f\"  Input: coordinates (N, 2)\")\n",
        "print(f\"  Output: u, v, p (N, 3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Transolver Model\n",
        "\n",
        "The model learns to partition mesh points into slices based on physical behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PhysicsAttentionLayer(nn.Module):\n",
        "    \"\"\"Physics-Attention layer from Transolver.\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, num_slices=8, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_slices = num_slices\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        \n",
        "        # Slice projection (this learns to partition the mesh!)\n",
        "        self.slice_proj = nn.Linear(dim, num_slices)\n",
        "        \n",
        "        # Multi-head attention projections\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, dim)\n",
        "        self.v_proj = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        \n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self._last_slice_weights = None  # Store for visualization\n",
        "    \n",
        "    def forward(self, x, return_slices=False):\n",
        "        B, N, D = x.shape\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        \n",
        "        # Step 1: SLICE - compute soft assignments\n",
        "        slice_logits = self.slice_proj(x)  # (B, N, M)\n",
        "        slice_weights = F.softmax(slice_logits, dim=-1)  # (B, N, M)\n",
        "        self._last_slice_weights = slice_weights.detach()  # Store for visualization\n",
        "        \n",
        "        # Step 2: AGGREGATE - compress to M tokens\n",
        "        slice_weights_t = slice_weights.transpose(1, 2)  # (B, M, N)\n",
        "        z = torch.bmm(slice_weights_t, x)  # (B, M, D)\n",
        "        z = z / (slice_weights_t.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "        \n",
        "        # Step 3: ATTEND - M×M attention (the cheap part!)\n",
        "        M = self.num_slices\n",
        "        Q = self.q_proj(z).view(B, M, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_proj(z).view(B, M, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_proj(z).view(B, M, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        z_prime = torch.matmul(attn, V)\n",
        "        z_prime = z_prime.transpose(1, 2).reshape(B, M, D)\n",
        "        z_prime = self.out_proj(z_prime)\n",
        "        \n",
        "        # Step 4: DESLICE - broadcast back to N points\n",
        "        out = torch.bmm(slice_weights, z_prime)  # (B, N, D)\n",
        "        \n",
        "        if return_slices:\n",
        "            return residual + out, slice_weights\n",
        "        return residual + out\n",
        "\n",
        "print(\"PhysicsAttentionLayer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTransolver(nn.Module):\n",
        "    \"\"\"Simplified Transolver for Stokes flow.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_dim=2, out_dim=3, hidden_dim=64, num_layers=3, num_slices=8):\n",
        "        super().__init__()\n",
        "        self.num_slices = num_slices\n",
        "        \n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            PhysicsAttentionLayer(hidden_dim, num_slices=num_slices, num_heads=4)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h)\n",
        "        return self.decoder(h)\n",
        "    \n",
        "    def get_slice_weights(self, x):\n",
        "        \"\"\"Get slice assignments from the first layer (for visualization).\"\"\"\n",
        "        h = self.embedding(x)\n",
        "        _, slice_weights = self.layers[0](h, return_slices=True)\n",
        "        return slice_weights\n",
        "\n",
        "# Create model\n",
        "NUM_SLICES = 8  # We'll visualize these\n",
        "model = SimpleTransolver(in_dim=2, out_dim=3, hidden_dim=64, num_layers=3, num_slices=NUM_SLICES).to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✓ Model created: {n_params:,} parameters\")\n",
        "print(f\"  - Hidden dim: 64\")\n",
        "print(f\"  - Layers: 3\")\n",
        "print(f\"  - Slices: {NUM_SLICES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train for 20 Epochs\n",
        "\n",
        "We'll train the model and track how the slice assignments evolve during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "losses = []\n",
        "slice_history = []  # Store slice assignments at different epochs\n",
        "\n",
        "# Get initial slice assignments (before training)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x = sample_coords.unsqueeze(0).to(device)\n",
        "    initial_slices = model.get_slice_weights(x)[0].cpu().numpy()\n",
        "    slice_history.append(('Epoch 0 (untrained)', initial_slices))\n",
        "\n",
        "print(\"Training Transolver...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    # Shuffle data each epoch\n",
        "    indices = np.random.permutation(len(train_data))\n",
        "    \n",
        "    for idx in indices:\n",
        "        data = train_data[idx]\n",
        "        coords = data['coords'].unsqueeze(0).to(device)  # (1, N, 2)\n",
        "        targets = data['targets'].unsqueeze(0).to(device)  # (1, N, 3)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(coords)\n",
        "        loss = criterion(pred, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_data)\n",
        "    losses.append(avg_loss)\n",
        "    \n",
        "    # Save slice assignments at key epochs\n",
        "    if (epoch + 1) in [10, 25, 50, 100]:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x = sample_coords.unsqueeze(0).to(device)\n",
        "            slices = model.get_slice_weights(x)[0].cpu().numpy()\n",
        "            slice_history.append((f'Epoch {epoch+1}', slices))\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS}: Loss = {avg_loss:.6f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"✓ Training complete! Final loss: {losses[-1]:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Learned Slice Assignments\n",
        "\n",
        "Let's see how the model learned to partition the mesh during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, NUM_EPOCHS+1), losses, 'b-o', linewidth=2, markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.semilogy(range(1, NUM_EPOCHS+1), losses, 'b-o', linewidth=2, markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss (log scale)')\n",
        "plt.title('Training Loss (Log Scale)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how slice assignments evolved during training\n",
        "coords_np = sample_coords.numpy()\n",
        "num_snapshots = len(slice_history)\n",
        "\n",
        "fig, axes = plt.subplots(1, num_snapshots, figsize=(4*num_snapshots, 4))\n",
        "\n",
        "for idx, (title, slice_weights) in enumerate(slice_history):\n",
        "    ax = axes[idx] if num_snapshots > 1 else axes\n",
        "    \n",
        "    # Get dominant slice for each point\n",
        "    dominant_slice = np.argmax(slice_weights, axis=1)\n",
        "    \n",
        "    # Plot mesh colored by slice\n",
        "    scatter = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=dominant_slice, \n",
        "                        cmap='tab10', s=6, alpha=0.7)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "plt.suptitle('Evolution of Learned Slice Assignments During Training', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice how slices become more spatially coherent as training progresses!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comparison: Learned slices vs actual physics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Get final slice assignments\n",
        "final_slices = slice_history[-1][1]\n",
        "dominant_slice = np.argmax(final_slices, axis=1)\n",
        "targets_np = sample_targets.numpy()\n",
        "\n",
        "# Top left: Velocity u\n",
        "ax = axes[0, 0]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=targets_np[:, 0], cmap='RdBu_r', s=6)\n",
        "ax.set_title('Ground Truth: Velocity u', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "\n",
        "# Top right: Velocity v\n",
        "ax = axes[0, 1]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=targets_np[:, 1], cmap='RdBu_r', s=6)\n",
        "ax.set_title('Ground Truth: Velocity v', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "\n",
        "# Bottom left: Pressure\n",
        "ax = axes[1, 0]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=targets_np[:, 2], cmap='viridis', s=6)\n",
        "ax.set_title('Ground Truth: Pressure p', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "\n",
        "# Bottom right: Learned slices\n",
        "ax = axes[1, 1]\n",
        "sc = ax.scatter(coords_np[:, 0], coords_np[:, 1], c=dominant_slice, cmap='tab10', s=6, alpha=0.8)\n",
        "ax.set_title(f'Learned Slice Assignments (M={NUM_SLICES})', fontsize=11)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "cbar = plt.colorbar(sc, ax=ax, shrink=0.8)\n",
        "cbar.set_label('Slice ID')\n",
        "\n",
        "plt.suptitle('Physics Fields vs Learned Slice Partitioning', fontsize=13, y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The model learns to group points with similar physical behavior into slices!\")\n",
        "\n",
        "# Visualize each of the 8 slices separately\n",
        "print(f\"\\n--- Visualizing all {NUM_SLICES} slices separately ---\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "slice_colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', \n",
        "                '#ff7f00', '#ffff33', '#a65628', '#f781bf']\n",
        "\n",
        "for slice_id in range(NUM_SLICES):\n",
        "    ax = axes[slice_id]\n",
        "    mask = dominant_slice == slice_id\n",
        "    count = np.sum(mask)\n",
        "    \n",
        "    # Plot all points faded\n",
        "    ax.scatter(coords_np[:, 0], coords_np[:, 1], c='lightgray', s=3, alpha=0.3)\n",
        "    # Highlight this slice's points\n",
        "    if count > 0:\n",
        "        ax.scatter(coords_np[mask, 0], coords_np[mask, 1], \n",
        "                  c=slice_colors[slice_id], s=8, alpha=0.8)\n",
        "    \n",
        "    ax.set_title(f'Slice {slice_id}: {count} points ({100*count/len(coords_np):.1f}%)', \n",
        "                fontsize=10, color=slice_colors[slice_id] if count > 0 else 'gray')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "plt.suptitle(f'All {NUM_SLICES} Learned Slices (After {NUM_EPOCHS} Epochs)', fontsize=13, y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print slice statistics\n",
        "print(\"\\nSlice Statistics:\")\n",
        "for i in range(NUM_SLICES):\n",
        "    count = np.sum(dominant_slice == i)\n",
        "    print(f\"  Slice {i}: {count:4d} points ({100*count/len(coords_np):5.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**What we demonstrated:**\n",
        "1. Trained a Transolver model on Stokes flow data for 20 epochs\n",
        "2. Visualized how **slice assignments evolve** during training\n",
        "3. The model learns to partition the mesh based on **physical behavior**, not just geometry\n",
        "\n",
        "**Key observations:**\n",
        "- Initially (untrained): slices are essentially random\n",
        "- After training: slices align with physics (e.g., inlet, wake, boundaries)\n",
        "- This learned partitioning enables efficient O(N·M) attention instead of O(N²)\n",
        "\n",
        "**References:**\n",
        "- [Transolver Paper](https://arxiv.org/abs/2402.02366)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
