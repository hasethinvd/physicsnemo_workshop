{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Transolver for Stokes Flow\n",
        "\n",
        "**Goal:** Apply Transolver to the same Stokes flow problem from Lab 2 and compare architectures.\n",
        "\n",
        "## Outline\n",
        "1. Quick Review: The Stokes Flow Problem\n",
        "2. Transolver Architecture Overview  \n",
        "3. PyTorch Implementation\n",
        "4. Comparison: MeshGraphNet vs Transolver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utils import softmax, download_stokes_dataset, load_stokes_sample\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Stokes Flow Problem (Recap)\n",
        "\n",
        "From Lab 2, we're solving:\n",
        "\n",
        "$$-\\nu \\Delta \\mathbf{u} + \\nabla p = 0, \\quad \\nabla \\cdot \\mathbf{u} = 0$$\n",
        "\n",
        "**Input:** Mesh with obstacle geometry  \n",
        "**Output:** Velocity field $(u, v)$ and pressure $p$\n",
        "\n",
        "**Challenge:** Variable mesh sizes, irregular geometries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Transolver Architecture\n",
        "\n",
        "```\n",
        "Input (N, d_in) → Embedding → [Physics-Attention Block] × L → Decoder → Output (N, d_out)\n",
        "                                     ↓\n",
        "                              Slice → Aggregate → Attend → Deslice\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PhysicsAttentionLayer(nn.Module):\n",
        "    \"\"\"Physics-Attention layer from Transolver.\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, num_slices=64, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_slices = num_slices\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        \n",
        "        # Slice projection\n",
        "        self.slice_proj = nn.Linear(dim, num_slices)\n",
        "        \n",
        "        # Multi-head attention projections\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, dim)\n",
        "        self.v_proj = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        \n",
        "        # Layer norm\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, N, dim) - mesh point features\n",
        "        Returns:\n",
        "            out: (batch, N, dim)\n",
        "        \"\"\"\n",
        "        B, N, D = x.shape\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        \n",
        "        # Step 1: SLICE - compute soft assignments\n",
        "        slice_logits = self.slice_proj(x)  # (B, N, M)\n",
        "        slice_weights = F.softmax(slice_logits, dim=-1)  # (B, N, M)\n",
        "        \n",
        "        # Step 2: AGGREGATE - compress to M tokens\n",
        "        # Weighted sum: (B, M, D)\n",
        "        slice_weights_t = slice_weights.transpose(1, 2)  # (B, M, N)\n",
        "        z = torch.bmm(slice_weights_t, x)  # (B, M, D)\n",
        "        # Normalize by sum of weights\n",
        "        z = z / (slice_weights_t.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "        \n",
        "        # Step 3: ATTEND - M×M multi-head attention\n",
        "        M = self.num_slices\n",
        "        Q = self.q_proj(z).view(B, M, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_proj(z).view(B, M, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_proj(z).view(B, M, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        z_prime = torch.matmul(attn, V)  # (B, heads, M, head_dim)\n",
        "        z_prime = z_prime.transpose(1, 2).reshape(B, M, D)  # (B, M, D)\n",
        "        z_prime = self.out_proj(z_prime)\n",
        "        \n",
        "        # Step 4: DESLICE - broadcast back to N points\n",
        "        out = torch.bmm(slice_weights, z_prime)  # (B, N, D)\n",
        "        \n",
        "        return residual + out\n",
        "\n",
        "# Test the layer\n",
        "B, N, D, M = 2, 500, 128, 32\n",
        "layer = PhysicsAttentionLayer(dim=D, num_slices=M).to(device)\n",
        "x_test = torch.randn(B, N, D).to(device)\n",
        "out = layer(x_test)\n",
        "print(f\"Input:  {x_test.shape}\")\n",
        "print(f\"Output: {out.shape}\")\n",
        "print(f\"✓ Physics-Attention layer works!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTransolver(nn.Module):\n",
        "    \"\"\"Simplified Transolver for Stokes flow.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_dim=2, out_dim=3, hidden_dim=128, num_layers=4, num_slices=64):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            PhysicsAttentionLayer(hidden_dim, num_slices=num_slices)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, N, in_dim) - mesh coordinates\n",
        "        Returns:\n",
        "            out: (batch, N, out_dim) - predicted fields (u, v, p)\n",
        "        \"\"\"\n",
        "        h = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h)\n",
        "        return self.decoder(h)\n",
        "\n",
        "# Test full model\n",
        "model = SimpleTransolver(in_dim=2, out_dim=3).to(device)\n",
        "coords = torch.randn(2, 500, 2).to(device)  # (batch, N points, 2D coords)\n",
        "pred = model(coords)\n",
        "print(f\"Coordinates: {coords.shape}\")\n",
        "print(f\"Prediction:  {pred.shape} (u, v, p for each point)\")\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameters:  {n_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparison: MeshGraphNet vs Transolver\n",
        "\n",
        "| Aspect | MeshGraphNet (Lab 2) | Transolver |\n",
        "|--------|---------------------|------------|\n",
        "| **Architecture** | Graph Neural Network | Transformer + Physics-Attention |\n",
        "| **Message Passing** | Local (neighbors only) | Global (via slices) |\n",
        "| **Complexity** | O(E) edges | O(N·M) |\n",
        "| **Inductive Bias** | Mesh connectivity | Learned physics slices |\n",
        "| **Scalability** | Good | Excellent |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Timing comparison (forward pass only)\n",
        "import time\n",
        "\n",
        "mesh_sizes = [500, 1000, 2000, 5000, 10000]\n",
        "transolver_times = []\n",
        "\n",
        "for N in mesh_sizes:\n",
        "    model = SimpleTransolver(in_dim=2, out_dim=3, num_slices=64).to(device)\n",
        "    x = torch.randn(1, N, 2).to(device)\n",
        "    \n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "    \n",
        "    # Time forward pass\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(x)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    elapsed = (time.time() - start) / 10\n",
        "    transolver_times.append(elapsed * 1000)  # ms\n",
        "    print(f\"N={N:5d}: {elapsed*1000:.2f} ms\")\n",
        "\n",
        "# Plot scaling\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(mesh_sizes, transolver_times, 'g-o', label='Transolver', linewidth=2)\n",
        "# Theoretical quadratic for comparison\n",
        "theoretical_quadratic = [t * (n/mesh_sizes[0])**2 for t, n in zip([transolver_times[0]], mesh_sizes)]\n",
        "plt.plot(mesh_sizes, [transolver_times[0] * (n/mesh_sizes[0])**2 for n in mesh_sizes], \n",
        "         'r--', label='O(N²) reference', alpha=0.5)\n",
        "plt.xlabel('Mesh Size (N)')\n",
        "plt.ylabel('Forward Pass Time (ms)')\n",
        "plt.title('Transolver Scaling: Near-Linear!')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Key Takeaways\n",
        "\n",
        "**What we learned:**\n",
        "1. **Physics-Attention** reduces complexity from O(N²) to O(N·M) by grouping points into learned \"slices\"\n",
        "2. **Transolver** stacks Physics-Attention layers to create a scalable transformer for PDEs\n",
        "3. Unlike GNNs, Transolver can capture **global** interactions efficiently\n",
        "\n",
        "**When to use what:**\n",
        "- **MeshGraphNet**: When mesh connectivity is important, moderate mesh sizes\n",
        "- **Transolver**: Large meshes, need for global interactions, variable mesh topologies\n",
        "\n",
        "**Next steps:**\n",
        "- Train Transolver on the Stokes dataset from Lab 2\n",
        "- Compare accuracy with MeshGraphNet\n",
        "- Experiment with number of slices (M) and layers\n",
        "\n",
        "## References\n",
        "- [Transolver Paper](https://arxiv.org/abs/2402.02366)\n",
        "- [MeshGraphNet Paper](https://arxiv.org/abs/2010.03409)\n",
        "- [PhysicsNeMo Documentation](https://github.com/NVIDIA/physicsnemo)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
