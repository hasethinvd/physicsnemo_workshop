{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Physics-Attention vs Standard Attention\n",
        "\n",
        "**Goal:** Understand why standard transformers are expensive for physics simulations and how Physics-Attention solves this.\n",
        "\n",
        "## Outline\n",
        "1. Load Sample Stokes Flow Data\n",
        "2. Standard Attention vs Physics-Attention\n",
        "3. Physics-Attention: The 4-Step Algorithm\n",
        "4. Visualizing Slice Assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utils import softmax, download_stokes_dataset, load_stokes_sample\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Download & Load Stokes Flow Data\n",
        "\n",
        "We'll use the Stokes flow dataset (same as Lab 2). Run the cell below to download if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset (if not present) and load a sample\n",
        "download_stokes_dataset()\n",
        "coords, u, v, p = load_stokes_sample()\n",
        "N_mesh = len(coords)\n",
        "\n",
        "# Visualize the Stokes flow data (uniform sizing)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 3.5))\n",
        "\n",
        "fields = [('Velocity u', u, 'RdBu_r'), ('Velocity v', v, 'RdBu_r'), ('Pressure p', p, 'viridis')]\n",
        "\n",
        "for ax, (title, field, cmap) in zip(axes, fields):\n",
        "    sc = ax.scatter(coords[:, 0], coords[:, 1], c=field, cmap=cmap, s=5)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_xlim(coords[:, 0].min() - 0.05, coords[:, 0].max() + 0.05)\n",
        "    ax.set_ylim(coords[:, 1].min() - 0.05, coords[:, 1].max() + 0.05)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.suptitle(f'Stokes Flow Data (N={N_mesh} mesh points)', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Loaded mesh with {N_mesh} points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Standard Attention vs Physics-Attention\n",
        "\n",
        "### Standard Self-Attention\n",
        "\n",
        "The transformer's attention mechanism computes:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "- **Q** (Query), **K** (Key), **V** (Value) are linear projections of input X ∈ ℝ^(N×C)\n",
        "- The **QKᵀ** term creates an **N × N** attention matrix\n",
        "- Every point attends to every other point → **O(N²) complexity**\n",
        "\n",
        "**Problem:** For a mesh with N=10,000 points, we need 100,000,000 operations per layer!\n",
        "\n",
        "### Physics-Attention (Transolver)\n",
        "\n",
        "Instead of N×N attention, Physics-Attention:\n",
        "1. Groups N mesh points into **M slices** (M << N, typically 8-64)\n",
        "2. Computes **M × M** attention between slice representations\n",
        "3. Broadcasts results back to N points\n",
        "\n",
        "**Result:** O(N·M + M²) ≈ **O(N·M)** complexity — orders of magnitude faster!\n",
        "\n",
        "| | Standard Attention | Physics-Attention |\n",
        "|---|---|---|\n",
        "| **Attention matrix** | N × N | M × M |\n",
        "| **Complexity** | O(N²) | O(N·M) |\n",
        "| **N=10,000, M=8** | 100,000,000 ops | 80,000 ops |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define both attention functions to compare\n",
        "N = N_mesh  # Use actual mesh size\n",
        "C = 8       # Feature dimension\n",
        "\n",
        "def standard_attention(X, d_k=8):\n",
        "    \"\"\"\n",
        "    Standard self-attention: O(N²) complexity\n",
        "    X: (N, C) input features\n",
        "    Returns: (N, C) output, attention matrix (N, N)\n",
        "    \"\"\"\n",
        "    # Linear projections (simplified - same weights for demo)\n",
        "    Q = X  # Query: (N, C)\n",
        "    K = X  # Key:   (N, C)  \n",
        "    V = X  # Value: (N, C)\n",
        "    \n",
        "    # THE EXPENSIVE PART: N×N attention matrix\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (N, N) - O(N²) operations!\n",
        "    attn = softmax(scores, axis=1)   # (N, N)\n",
        "    \n",
        "    output = attn @ V  # (N, C)\n",
        "    return output, attn\n",
        "\n",
        "def physics_attention(X, M, d_k=8):\n",
        "    \"\"\"\n",
        "    Physics-Attention: O(N·M) complexity where M << N\n",
        "    X: (N, C) input features\n",
        "    M: number of slices\n",
        "    Returns: (N, C) output, slice weights (N, M), slice attention (M, M)\n",
        "    \"\"\"\n",
        "    N, C = X.shape\n",
        "    \n",
        "    # Step 1: SLICE - assign N points to M slices (soft assignment)\n",
        "    W_slice = np.random.randn(C, M) * 0.5  # Learnable in real model\n",
        "    slice_weights = softmax(X @ W_slice, axis=1)  # (N, M) - O(N·M)\n",
        "    \n",
        "    # Step 2: AGGREGATE - compress each slice into single token\n",
        "    slice_tokens = slice_weights.T @ X  # (M, C) - weighted sum per slice\n",
        "    \n",
        "    # Step 3: ATTEND - M×M attention (THE CHEAP PART!)\n",
        "    Q = slice_tokens  # (M, C)\n",
        "    K = slice_tokens  # (M, C)\n",
        "    V = slice_tokens  # (M, C)\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (M, M) - only O(M²) operations!\n",
        "    slice_attn = softmax(scores, axis=1)  # (M, M)\n",
        "    attended = slice_attn @ V  # (M, C)\n",
        "    \n",
        "    # Step 4: DESLICE - broadcast back to N points\n",
        "    output = slice_weights @ attended  # (N, C) - O(N·M)\n",
        "    \n",
        "    return output, slice_weights, slice_attn\n",
        "\n",
        "# Create sample input\n",
        "X = np.random.randn(N, C)\n",
        "\n",
        "# Compare costs for different M values\n",
        "print(\"=\"*70)\n",
        "print(f\"COST COMPARISON: Standard vs Physics-Attention (N = {N:,} mesh points)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Method':<25} {'Attention Size':<18} {'Operations':<15} {'Speedup':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Standard attention\n",
        "std_out, std_attn = standard_attention(X)\n",
        "std_ops = N * N\n",
        "print(f\"{'Standard Attention':<25} {f'{N}×{N}':<18} {std_ops:,} ops{'':<5} {'1x (baseline)':<10}\")\n",
        "\n",
        "# Physics attention with different M\n",
        "for M in [4, 8, 16, 32, 64]:\n",
        "    phys_out, slice_w, slice_attn = physics_attention(X, M)\n",
        "    # Total ops: N·M (slice) + M² (attend) + N·M (deslice) ≈ 2·N·M + M²\n",
        "    phys_ops = 2 * N * M + M * M\n",
        "    speedup = std_ops / phys_ops\n",
        "    print(f\"{'Physics-Attention M='+str(M):<25} {f'{M}×{M}':<18} {phys_ops:,} ops{'':<5} {speedup:.0f}x faster\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(f\"\\n✓ With M=8 slices, we get ~{std_ops // (2*N*8 + 64):.0f}x speedup!\")\n",
        "print(\"✓ The key: M×M attention instead of N×N\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Physics-Attention: The 4-Step Solution\n",
        "\n",
        "### What are \"Slices\"?\n",
        "\n",
        "In physics simulations, different regions of a mesh often have similar physical behavior:\n",
        "- **Inlet region**: Smooth laminar flow\n",
        "- **Obstacle wake**: Turbulent/recirculating flow  \n",
        "- **Boundary layer**: High gradients near walls\n",
        "- **Far-field**: Nearly uniform flow\n",
        "\n",
        "**Slices** are learned groupings that cluster mesh points with similar physics. Instead of every point attending to every other point (N×N), we:\n",
        "1. Group points into M slices (soft assignment via learned weights)\n",
        "2. Compute attention only between slice representations (M×M)\n",
        "\n",
        "### The 4-Step Algorithm\n",
        "\n",
        "```\n",
        "Input: X ∈ R^(N×C)  (N mesh points, C features)\n",
        "       W ∈ R^(C×M)  (learnable slice weights)\n",
        "\n",
        "Step 1 - SLICE:     S = softmax(X @ W)           → S ∈ R^(N×M)  (assignment weights)\n",
        "Step 2 - AGGREGATE: Z = S^T @ X                  → Z ∈ R^(M×C)  (slice tokens)\n",
        "Step 3 - ATTEND:    Z' = Attention(Z, Z, Z)      → Z' ∈ R^(M×C) (M×M attention!)\n",
        "Step 4 - DESLICE:   Y = S @ Z'                   → Y ∈ R^(N×C)  (broadcast back)\n",
        "\n",
        "Output: Y ∈ R^(N×C)\n",
        "```\n",
        "\n",
        "**Key insight:** Step 3 is O(M²) instead of O(N²), and M is typically 8-64 while N can be 10,000+!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create physics-based features from our mesh (coords + physics values)\n",
        "C = 8  # Feature dimension\n",
        "features = np.column_stack([\n",
        "    coords,  # x, y coordinates\n",
        "    u.reshape(-1, 1),  # velocity u\n",
        "    v.reshape(-1, 1),  # velocity v  \n",
        "    p.reshape(-1, 1),  # pressure\n",
        "    np.random.randn(N, C-5)  # padding to get C features\n",
        "])\n",
        "\n",
        "print(f\"✓ Created feature matrix: {features.shape} (N={N} points, C={C} features)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. How Mesh Points Get Distributed to Slices\n",
        "\n",
        "Let's visualize exactly how mesh points get assigned to slices. Each point gets a **soft assignment weight** to each slice (values sum to 1). The point \"belongs\" most strongly to the slice with highest weight.\n",
        "\n",
        "### Step-by-Step with M=3 Slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize slice assignments with M=3 slices\n",
        "M_demo = 3\n",
        "np.random.seed(123)\n",
        "\n",
        "# Compute slice assignments\n",
        "W_demo = np.random.randn(C, M_demo) * 0.8\n",
        "slice_weights = softmax(features @ W_demo, axis=1)  # (N, M) soft assignment\n",
        "dominant_slice = np.argmax(slice_weights, axis=1)   # Hard assignment for visualization\n",
        "\n",
        "slice_colors = ['#e41a1c', '#377eb8', '#4daf4a']  # Red, Blue, Green\n",
        "counts = [np.sum(dominant_slice == i) for i in range(M_demo)]\n",
        "\n",
        "# Simple visualization: mesh + attention matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Left: Mesh colored by slice assignment\n",
        "ax1 = axes[0]\n",
        "for i in range(M_demo):\n",
        "    mask = dominant_slice == i\n",
        "    ax1.scatter(coords[mask, 0], coords[mask, 1], c=slice_colors[i], \n",
        "               s=8, alpha=0.7, label=f'Slice {i} ({counts[i]} pts)')\n",
        "ax1.set_title(f'Mesh Points Assigned to {M_demo} Slices', fontsize=12)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_aspect('equal')\n",
        "ax1.legend(loc='upper right')\n",
        "\n",
        "# Right: The M×M attention matrix (the cheap part!)\n",
        "ax2 = axes[1]\n",
        "phys_attn = softmax(np.random.randn(M_demo, M_demo), axis=1)\n",
        "im = ax2.imshow(phys_attn, cmap='Greens', vmin=0, vmax=1)\n",
        "ax2.set_title(f'Physics-Attention: {M_demo}×{M_demo} = {M_demo**2} ops\\n(instead of {N}×{N} = {N*N:,} ops)', fontsize=11)\n",
        "ax2.set_xlabel('Key Slice')\n",
        "ax2.set_ylabel('Query Slice')\n",
        "ax2.set_xticks(range(M_demo))\n",
        "ax2.set_yticks(range(M_demo))\n",
        "plt.colorbar(im, ax=ax2, shrink=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Cost reduction: {N*N:,} → {M_demo**2} = {N*N // M_demo**2:,}x fewer operations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effect of Different Slice Counts (M)\n",
        "\n",
        "More slices = finer grouping but higher cost. Typical values: M=8 to M=64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare M=4, M=8, M=16 slice partitioning\n",
        "slice_configs = [4, 8, 16]\n",
        "fig_compare, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, num_slices in enumerate(slice_configs):\n",
        "    ax = axes[idx]\n",
        "    np.random.seed(42 + idx)  # Different seed for variety\n",
        "    \n",
        "    # Compute slice assignments\n",
        "    W = np.random.randn(C, num_slices) * 0.6\n",
        "    logits = features @ W\n",
        "    weights = softmax(logits, axis=1)\n",
        "    dominant_slice = np.argmax(weights, axis=1)\n",
        "    \n",
        "    # Plot mesh colored by slice\n",
        "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=dominant_slice, \n",
        "                        cmap='tab10' if num_slices <= 10 else 'tab20',\n",
        "                        s=6, alpha=0.7)\n",
        "    \n",
        "    # Calculate cost reduction\n",
        "    cost_reduction = N*N // (num_slices**2)\n",
        "    ax.set_title(f'M = {num_slices} slices\\nAttention: {num_slices}×{num_slices}={num_slices**2} ops\\n({cost_reduction:,}x cheaper)', \n",
        "                fontsize=10)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "    plt.colorbar(scatter, ax=ax, label='Slice ID', shrink=0.8)\n",
        "\n",
        "plt.suptitle('Trade-off: More Slices = Finer Resolution but Higher Cost', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTypical M values in Transolver: 8-64 (paper uses M=32 or M=64)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Aspect | Standard Attention | Physics-Attention |\n",
        "|--------|-------------------|-------------------|\n",
        "| **Complexity** | O(N²) — expensive! | O(N·M) — efficient! |\n",
        "| **Attention matrix** | N×N | M×M (M≈64) |\n",
        "| **Grouping** | All-to-all | Learned slices |\n",
        "\n",
        "**Key Takeaway:** Physics-Attention reduces cost by grouping mesh points into M learned \"slices\" and performing attention between these compressed representations.\n",
        "\n",
        "**Next:** Notebook 2 shows the full Transolver architecture in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
