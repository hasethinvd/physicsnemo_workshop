{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Physics-Attention vs Standard Attention\n",
        "\n",
        "**Goal:** Understand why standard transformers are expensive for physics simulations and how Physics-Attention solves this.\n",
        "\n",
        "## Outline\n",
        "1. Load Sample Stokes Flow Data\n",
        "2. Standard Attention vs Physics-Attention\n",
        "3. Physics-Attention: The 4-Step Algorithm\n",
        "4. Visualizing Slice Assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from utils import softmax, download_stokes_dataset, load_stokes_sample\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Download & Load Stokes Flow Data\n",
        "\n",
        "We'll use the Stokes flow dataset (same as Lab 2). Run the cell below to download if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset (if not present) and load a sample\n",
        "download_stokes_dataset()\n",
        "coords, u, v, p = load_stokes_sample()\n",
        "N_mesh = len(coords)\n",
        "\n",
        "# Visualize the Stokes flow data (uniform sizing)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 3.5))\n",
        "\n",
        "fields = [('Velocity u', u, 'RdBu_r'), ('Velocity v', v, 'RdBu_r'), ('Pressure p', p, 'viridis')]\n",
        "\n",
        "for ax, (title, field, cmap) in zip(axes, fields):\n",
        "    sc = ax.scatter(coords[:, 0], coords[:, 1], c=field, cmap=cmap, s=5)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_xlim(coords[:, 0].min() - 0.05, coords[:, 0].max() + 0.05)\n",
        "    ax.set_ylim(coords[:, 1].min() - 0.05, coords[:, 1].max() + 0.05)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.suptitle(f'Stokes Flow Data (N={N_mesh} mesh points)', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Loaded mesh with {N_mesh} points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Standard Attention vs Physics-Attention\n",
        "\n",
        "### Standard Self-Attention\n",
        "\n",
        "The transformer's attention mechanism computes:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where:\n",
        "- **Q** (Query), **K** (Key), **V** (Value) are linear projections of input X ∈ ℝ^(N×C)\n",
        "- The **QKᵀ** term creates an **N × N** attention matrix\n",
        "- Every point attends to every other point → **O(N²) complexity**\n",
        "\n",
        "**Problem:** For a mesh with N=10,000 points, we need 100,000,000 operations per layer!\n",
        "\n",
        "### Physics-Attention (Transolver)\n",
        "\n",
        "Instead of N×N attention, Physics-Attention:\n",
        "1. Groups N mesh points into **M slices** (M << N, typically 8-64)\n",
        "2. Computes **M × M** attention between slice representations\n",
        "3. Broadcasts results back to N points\n",
        "\n",
        "**Result:** O(N·M + M²) ≈ **O(N·M)** complexity — orders of magnitude faster!\n",
        "\n",
        "| | Standard Attention | Physics-Attention |\n",
        "|---|---|---|\n",
        "| **Attention matrix** | N × N | M × M |\n",
        "| **Complexity** | O(N²) | O(N·M) |\n",
        "| **N=10,000, M=8** | 100,000,000 ops | 80,000 ops |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define both attention functions to compare\n",
        "N = N_mesh  # Use actual mesh size\n",
        "C = 8       # Feature dimension\n",
        "\n",
        "def standard_attention(X, d_k=8):\n",
        "    \"\"\"\n",
        "    Standard self-attention: O(N²) complexity\n",
        "    X: (N, C) input features\n",
        "    Returns: (N, C) output, attention matrix (N, N)\n",
        "    \"\"\"\n",
        "    # Linear projections (simplified - same weights for demo)\n",
        "    Q = X  # Query: (N, C)\n",
        "    K = X  # Key:   (N, C)  \n",
        "    V = X  # Value: (N, C)\n",
        "    \n",
        "    # THE EXPENSIVE PART: N×N attention matrix\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (N, N) - O(N²) operations!\n",
        "    attn = softmax(scores, axis=1)   # (N, N)\n",
        "    \n",
        "    output = attn @ V  # (N, C)\n",
        "    return output, attn\n",
        "\n",
        "def physics_attention(X, M, d_k=8):\n",
        "    \"\"\"\n",
        "    Physics-Attention: O(N·M) complexity where M << N\n",
        "    X: (N, C) input features\n",
        "    M: number of slices\n",
        "    Returns: (N, C) output, slice weights (N, M), slice attention (M, M)\n",
        "    \"\"\"\n",
        "    N, C = X.shape\n",
        "    \n",
        "    # Step 1: SLICE - assign N points to M slices (soft assignment)\n",
        "    W_slice = np.random.randn(C, M) * 0.5  # Learnable in real model\n",
        "    slice_weights = softmax(X @ W_slice, axis=1)  # (N, M) - O(N·M)\n",
        "    \n",
        "    # Step 2: AGGREGATE - compress each slice into single token\n",
        "    slice_tokens = slice_weights.T @ X  # (M, C) - weighted sum per slice\n",
        "    \n",
        "    # Step 3: ATTEND - M×M attention (THE CHEAP PART!)\n",
        "    Q = slice_tokens  # (M, C)\n",
        "    K = slice_tokens  # (M, C)\n",
        "    V = slice_tokens  # (M, C)\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (M, M) - only O(M²) operations!\n",
        "    slice_attn = softmax(scores, axis=1)  # (M, M)\n",
        "    attended = slice_attn @ V  # (M, C)\n",
        "    \n",
        "    # Step 4: DESLICE - broadcast back to N points\n",
        "    output = slice_weights @ attended  # (N, C) - O(N·M)\n",
        "    \n",
        "    return output, slice_weights, slice_attn\n",
        "\n",
        "# Create sample input\n",
        "X = np.random.randn(N, C)\n",
        "\n",
        "# Compare costs for different M values\n",
        "print(\"=\"*70)\n",
        "print(f\"COST COMPARISON: Standard vs Physics-Attention (N = {N:,} mesh points)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Method':<25} {'Attention Size':<18} {'Operations':<15} {'Speedup':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Standard attention\n",
        "std_out, std_attn = standard_attention(X)\n",
        "std_ops = N * N\n",
        "print(f\"{'Standard Attention':<25} {f'{N}×{N}':<18} {std_ops:,} ops{'':<5} {'1x (baseline)':<10}\")\n",
        "\n",
        "# Physics attention with different M\n",
        "for M in [4, 8, 16, 32, 64]:\n",
        "    phys_out, slice_w, slice_attn = physics_attention(X, M)\n",
        "    # Total ops: N·M (slice) + M² (attend) + N·M (deslice) ≈ 2·N·M + M²\n",
        "    phys_ops = 2 * N * M + M * M\n",
        "    speedup = std_ops / phys_ops\n",
        "    print(f\"{'Physics-Attention M='+str(M):<25} {f'{M}×{M}':<18} {phys_ops:,} ops{'':<5} {speedup:.0f}x faster\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(f\"\\n✓ With M=8 slices, we get ~{std_ops // (2*N*8 + 64):.0f}x speedup!\")\n",
        "print(\"✓ The key: M×M attention instead of N×N\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the attention matrices: Standard (N×N) vs Physics (M×M)\n",
        "M_vis = 8\n",
        "_, _, phys_attn_vis = physics_attention(X, M_vis)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Standard attention (subsampled for display)\n",
        "ax1 = axes[0]\n",
        "subsample = min(100, N)\n",
        "im1 = ax1.imshow(std_attn[:subsample, :subsample], cmap='Reds', aspect='equal')\n",
        "ax1.set_title(f'Standard Attention\\n{N}×{N} = {N*N:,} ops', fontsize=11, color='red')\n",
        "ax1.set_xlabel('Key (mesh points)')\n",
        "ax1.set_ylabel('Query (mesh points)')\n",
        "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
        "\n",
        "# Physics attention (M×M)\n",
        "ax2 = axes[1]\n",
        "im2 = ax2.imshow(phys_attn_vis, cmap='Greens', aspect='equal')\n",
        "ax2.set_title(f'Physics-Attention\\n{M_vis}×{M_vis} = {M_vis**2} ops', fontsize=11, color='green')\n",
        "ax2.set_xlabel('Key (slices)')\n",
        "ax2.set_ylabel('Query (slices)')\n",
        "ax2.set_xticks(range(M_vis))\n",
        "ax2.set_yticks(range(M_vis))\n",
        "plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
        "\n",
        "# Size comparison\n",
        "ax3 = axes[2]\n",
        "ax3.bar(['Standard\\nN×N', 'Physics\\nM×M'], [N*N, M_vis*M_vis], color=['red', 'green'], alpha=0.7)\n",
        "ax3.set_ylabel('Attention Matrix Size')\n",
        "ax3.set_title(f'Size Comparison\\n({N*N // (M_vis**2):,}x reduction)', fontsize=11)\n",
        "ax3.set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Physics-Attention: The 4-Step Solution\n",
        "\n",
        "### What are \"Slices\"?\n",
        "\n",
        "In physics simulations, different regions of a mesh often have similar physical behavior:\n",
        "- **Inlet region**: Smooth laminar flow\n",
        "- **Obstacle wake**: Turbulent/recirculating flow  \n",
        "- **Boundary layer**: High gradients near walls\n",
        "- **Far-field**: Nearly uniform flow\n",
        "\n",
        "**Slices** are learned groupings that cluster mesh points with similar physics. Instead of every point attending to every other point (N×N), we:\n",
        "1. Group points into M slices (soft assignment via learned weights)\n",
        "2. Compute attention only between slice representations (M×M)\n",
        "\n",
        "### The 4-Step Algorithm\n",
        "\n",
        "```\n",
        "Input: X ∈ R^(N×C)  (N mesh points, C features)\n",
        "       W ∈ R^(C×M)  (learnable slice weights)\n",
        "\n",
        "Step 1 - SLICE:     S = softmax(X @ W)           → S ∈ R^(N×M)  (assignment weights)\n",
        "Step 2 - AGGREGATE: Z = S^T @ X                  → Z ∈ R^(M×C)  (slice tokens)\n",
        "Step 3 - ATTEND:    Z' = Attention(Z, Z, Z)      → Z' ∈ R^(M×C) (M×M attention!)\n",
        "Step 4 - DESLICE:   Y = S @ Z'                   → Y ∈ R^(N×C)  (broadcast back)\n",
        "\n",
        "Output: Y ∈ R^(N×C)\n",
        "```\n",
        "\n",
        "**Key insight:** Step 3 is O(M²) instead of O(N²), and M is typically 8-64 while N can be 10,000+!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create physics-based features from our mesh (coords + physics values)\n",
        "C = 8  # Feature dimension\n",
        "features = np.column_stack([\n",
        "    coords,  # x, y coordinates\n",
        "    u.reshape(-1, 1),  # velocity u\n",
        "    v.reshape(-1, 1),  # velocity v  \n",
        "    p.reshape(-1, 1),  # pressure\n",
        "    np.random.randn(N, C-5)  # padding to get C features\n",
        "])\n",
        "\n",
        "# Simulate slice assignment (learned weights in real model)\n",
        "W_slice = np.random.randn(C, M) * 0.5\n",
        "slice_logits = features @ W_slice\n",
        "slice_weights = softmax(slice_logits, axis=1)  # (N, M) - soft assignment\n",
        "\n",
        "# Simulate M×M attention\n",
        "attn_matrix = softmax(np.random.randn(M, M), axis=1)\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Standard attention matrix (subsampled)\n",
        "ax1 = axes[0]\n",
        "subsample = min(80, N)\n",
        "std_attn = softmax(np.random.randn(subsample, subsample), axis=1)\n",
        "im1 = ax1.imshow(std_attn, cmap='Reds')\n",
        "ax1.set_title(f'Standard Attention\\n({N}×{N} = {N*N:,} ops)', fontsize=11)\n",
        "ax1.set_xlabel('Key points')\n",
        "ax1.set_ylabel('Query points')\n",
        "\n",
        "# Slice assignment matrix\n",
        "ax2 = axes[1]\n",
        "# Sort by x-coordinate for visualization\n",
        "sort_idx = np.argsort(coords[:, 0])\n",
        "im2 = ax2.imshow(slice_weights[sort_idx[:200]].T, aspect='auto', cmap='Greens')\n",
        "ax2.set_title('Slice Assignments\\n(N points → M slices)', fontsize=11)\n",
        "ax2.set_xlabel('Mesh points (sorted by x)')\n",
        "ax2.set_ylabel(f'Slices (M={M})')\n",
        "plt.colorbar(im2, ax=ax2)\n",
        "\n",
        "# Physics-Attention: M×M\n",
        "ax3 = axes[2]\n",
        "im3 = ax3.imshow(attn_matrix, cmap='Greens')\n",
        "ax3.set_title(f'Physics-Attention\\n({M}×{M} = {M*M} ops)', fontsize=11)\n",
        "ax3.set_xlabel('Key slices')\n",
        "ax3.set_ylabel('Query slices')\n",
        "plt.colorbar(im3, ax=ax3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Cost reduction: {N*N:,} → {M*M} = {N*N // (M*M):,}x fewer attention operations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. How Mesh Points Get Distributed to Slices\n",
        "\n",
        "Let's visualize exactly how mesh points get assigned to slices. Each point gets a **soft assignment weight** to each slice (values sum to 1). The point \"belongs\" most strongly to the slice with highest weight.\n",
        "\n",
        "### Step-by-Step with M=3 Slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed visualization with M=3 slices\n",
        "M_demo = 3\n",
        "np.random.seed(123)  # For reproducibility\n",
        "\n",
        "# Step 1: Compute slice assignment weights\n",
        "W_demo = np.random.randn(C, M_demo) * 0.8\n",
        "slice_logits = features @ W_demo\n",
        "slice_weights = softmax(slice_logits, axis=1)  # Shape: (N, 3)\n",
        "dominant_slice = np.argmax(slice_weights, axis=1)\n",
        "\n",
        "# Count points per slice\n",
        "slice_colors = ['#e41a1c', '#377eb8', '#4daf4a']  # Red, Blue, Green\n",
        "slice_names = ['Slice 0 (Red)', 'Slice 1 (Blue)', 'Slice 2 (Green)']\n",
        "counts = [np.sum(dominant_slice == i) for i in range(M_demo)]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"SLICE ASSIGNMENT SUMMARY (M={M_demo} slices)\")\n",
        "print(\"=\"*60)\n",
        "for i, (name, count) in enumerate(zip(slice_names, counts)):\n",
        "    pct = 100 * count / N\n",
        "    print(f\"  {name}: {count:,} points ({pct:.1f}%)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Row 1: Show individual slices\n",
        "for i in range(M_demo):\n",
        "    ax = fig.add_subplot(2, 4, i+1)\n",
        "    mask = dominant_slice == i\n",
        "    \n",
        "    # Plot all points faded\n",
        "    ax.scatter(coords[:, 0], coords[:, 1], c='lightgray', s=3, alpha=0.3)\n",
        "    # Highlight this slice's points\n",
        "    ax.scatter(coords[mask, 0], coords[mask, 1], c=slice_colors[i], s=8, alpha=0.8)\n",
        "    \n",
        "    ax.set_title(f'Slice {i}\\n({counts[i]:,} points)', fontsize=11, color=slice_colors[i])\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "# Row 1, Col 4: All slices combined\n",
        "ax_combined = fig.add_subplot(2, 4, 4)\n",
        "for i in range(M_demo):\n",
        "    mask = dominant_slice == i\n",
        "    ax_combined.scatter(coords[mask, 0], coords[mask, 1], \n",
        "                       c=slice_colors[i], s=6, alpha=0.7, label=f'Slice {i}')\n",
        "ax_combined.set_title('All 3 Slices Combined', fontsize=11)\n",
        "ax_combined.set_xlabel('x')\n",
        "ax_combined.set_ylabel('y')\n",
        "ax_combined.set_aspect('equal')\n",
        "ax_combined.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "# Row 2, Col 1: Soft assignment weights heatmap\n",
        "ax_weights = fig.add_subplot(2, 4, 5)\n",
        "# Sort points by dominant slice for visualization\n",
        "sort_idx = np.lexsort((coords[np.arange(N), 0], dominant_slice))\n",
        "im = ax_weights.imshow(slice_weights[sort_idx].T, aspect='auto', cmap='YlOrRd',\n",
        "                       extent=[0, N, -0.5, M_demo-0.5])\n",
        "ax_weights.set_title('Soft Assignment Weights\\n(each column sums to 1)', fontsize=10)\n",
        "ax_weights.set_xlabel(f'Mesh Points (sorted, N={N})')\n",
        "ax_weights.set_ylabel('Slice ID')\n",
        "ax_weights.set_yticks([0, 1, 2])\n",
        "plt.colorbar(im, ax=ax_weights, label='Weight')\n",
        "\n",
        "# Row 2, Col 2: Bar chart of distribution\n",
        "ax_bar = fig.add_subplot(2, 4, 6)\n",
        "bars = ax_bar.bar(range(M_demo), counts, color=slice_colors, edgecolor='black')\n",
        "ax_bar.set_xlabel('Slice ID')\n",
        "ax_bar.set_ylabel('Number of Points')\n",
        "ax_bar.set_title('Points per Slice', fontsize=11)\n",
        "ax_bar.set_xticks(range(M_demo))\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax_bar.annotate(f'{count}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
        "                   ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Row 2, Col 3-4: Example point assignment\n",
        "ax_example = fig.add_subplot(2, 4, 7)\n",
        "# Pick 5 example points (one from each region)\n",
        "example_indices = np.random.choice(N, 5, replace=False)\n",
        "example_weights = slice_weights[example_indices]\n",
        "\n",
        "x_pos = np.arange(5)\n",
        "width = 0.25\n",
        "for i in range(M_demo):\n",
        "    ax_example.bar(x_pos + i*width, example_weights[:, i], width, \n",
        "                  color=slice_colors[i], label=f'Slice {i}', alpha=0.8)\n",
        "ax_example.set_xlabel('Example Points')\n",
        "ax_example.set_ylabel('Assignment Weight')\n",
        "ax_example.set_title('Soft Weights for 5 Sample Points\\n(each point sums to 1)', fontsize=10)\n",
        "ax_example.set_xticks(x_pos + width)\n",
        "ax_example.set_xticklabels([f'P{i}' for i in range(5)])\n",
        "ax_example.legend(fontsize=8)\n",
        "ax_example.set_ylim(0, 1)\n",
        "\n",
        "# Row 2, Col 4: The resulting M×M attention\n",
        "ax_attn = fig.add_subplot(2, 4, 8)\n",
        "phys_attn = softmax(np.random.randn(M_demo, M_demo), axis=1)\n",
        "im_attn = ax_attn.imshow(phys_attn, cmap='Greens', vmin=0, vmax=1)\n",
        "ax_attn.set_title(f'Physics-Attention Matrix\\n(only {M_demo}×{M_demo}={M_demo**2} ops!)', fontsize=10)\n",
        "ax_attn.set_xlabel('Key Slice')\n",
        "ax_attn.set_ylabel('Query Slice')\n",
        "ax_attn.set_xticks(range(M_demo))\n",
        "ax_attn.set_yticks(range(M_demo))\n",
        "plt.colorbar(im_attn, ax=ax_attn)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✓ Instead of {N}×{N}={N*N:,} attention ops, we only need {M_demo}×{M_demo}={M_demo**2}!\")\n",
        "print(f\"✓ Cost reduction: {N*N // M_demo**2:,}x fewer operations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effect of Different Slice Counts (M)\n",
        "\n",
        "More slices = finer grouping but higher cost. Typical values: M=8 to M=64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare M=4, M=8, M=16 slice partitioning\n",
        "slice_configs = [4, 8, 16]\n",
        "fig_compare, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, num_slices in enumerate(slice_configs):\n",
        "    ax = axes[idx]\n",
        "    np.random.seed(42 + idx)  # Different seed for variety\n",
        "    \n",
        "    # Compute slice assignments\n",
        "    W = np.random.randn(C, num_slices) * 0.6\n",
        "    logits = features @ W\n",
        "    weights = softmax(logits, axis=1)\n",
        "    dominant_slice = np.argmax(weights, axis=1)\n",
        "    \n",
        "    # Plot mesh colored by slice\n",
        "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=dominant_slice, \n",
        "                        cmap='tab10' if num_slices <= 10 else 'tab20',\n",
        "                        s=6, alpha=0.7)\n",
        "    \n",
        "    # Calculate cost reduction\n",
        "    cost_reduction = N*N // (num_slices**2)\n",
        "    ax.set_title(f'M = {num_slices} slices\\nAttention: {num_slices}×{num_slices}={num_slices**2} ops\\n({cost_reduction:,}x cheaper)', \n",
        "                fontsize=10)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "    plt.colorbar(scatter, ax=ax, label='Slice ID', shrink=0.8)\n",
        "\n",
        "plt.suptitle('Trade-off: More Slices = Finer Resolution but Higher Cost', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTypical M values in Transolver: 8-64 (paper uses M=32 or M=64)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Aspect | Standard Attention | Physics-Attention |\n",
        "|--------|-------------------|-------------------|\n",
        "| **Complexity** | O(N²) — expensive! | O(N·M) — efficient! |\n",
        "| **Attention matrix** | N×N | M×M (M≈64) |\n",
        "| **Grouping** | All-to-all | Learned slices |\n",
        "\n",
        "**Key Takeaway:** Physics-Attention reduces cost by grouping mesh points into M learned \"slices\" and performing attention between these compressed representations.\n",
        "\n",
        "**Next:** Notebook 2 shows the full Transolver architecture in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
