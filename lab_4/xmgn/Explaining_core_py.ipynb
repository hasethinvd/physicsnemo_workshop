{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e18424",
   "metadata": {},
   "source": [
    "# Example 1: 2D Waterflood Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ac7af",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding preprocessor.py \n",
    "\n",
    "Before running preprocessing, let's understand what the `preprocessor.py` script actually does. This will help you understand the outputs and troubleshoot any issues.\n",
    "\n",
    "#### Main Classes and Components\n",
    "\n",
    "**1. `ReservoirPreprocessor` - Main orchestrator class**\n",
    "\n",
    "```python\n",
    "class ReservoirPreprocessor:\n",
    "    def __init__(self, cfg):\n",
    "        # Set up directories\n",
    "        self.dataset_dir = get_dataset_dir(cfg)\n",
    "        self.graphs_dir = os.path.join(self.dataset_dir, \"graphs\")\n",
    "        self.partitions_dir = os.path.join(self.dataset_dir, \"partitions\")\n",
    "        self.stats_file = os.path.join(self.dataset_dir, \"global_stats.json\")\n",
    "```\n",
    "\n",
    "#### The Preprocessing Pipeline (execute() method)\n",
    "\n",
    "The preprocessing happens in **5 main steps**:\n",
    "\n",
    "##### **Step 1: Create Raw Graphs** (unless `skip_graphs=True`)\n",
    "\n",
    "```python\n",
    "processor = ReservoirGraphBuilder(self.cfg)\n",
    "self.generated_files = processor.execute()\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Reads ECLIPSE binary files (`.INIT`, `.EGRID`, `.UNRST`)\n",
    "- Extracts grid geometry and cell connections\n",
    "- Builds graph structure with nodes (cells) and edges (connections)\n",
    "- Creates training sequences: Input (t-2, t-1, t) → Target (t+1)\n",
    "- Saves raw graphs to `graphs/` directory\n",
    "\n",
    "Output: `graph_name_timestep.pt` files (e.g., `CASE_001_000.pt`)\n",
    "\n",
    "##### **Step 2: Create Partitions from Graphs**\n",
    "\n",
    "```python\n",
    "self.create_partitions_from_graphs(graph_file_list=self.graph_file_list)\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Loads each raw graph\n",
    "- Uses **METIS** (or fallback: simple sequential partitioning) to divide graph into `num_partitions` subgraphs\n",
    "- For each partition:\n",
    "  - Extract inner nodes (the actual partition)\n",
    "  - Add **halo region** (neighboring nodes) for communication\n",
    "  - Creates partition object with: `edge_index`, `node_features`, `inner_node` indices\n",
    "- Saves partitions for each graph\n",
    "\n",
    "Output: `partitions_graph_name_timestep.pt` files\n",
    "\n",
    "**Why Partition?**\n",
    "- Large graphs may not fit in single GPU memory\n",
    "- Enables multi-GPU training (each GPU handles different partitions)\n",
    "- Halo regions allow information exchange between partitions\n",
    "\n",
    "##### **Step 3: Split Samples by Case**\n",
    "\n",
    "```python\n",
    "splits = self.split_samples_by_case(\n",
    "    train_ratio=0.8, val_ratio=0.1, test_ratio=0.1\n",
    ")\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Groups all timesteps belonging to the same simulation case\n",
    "- Randomly assigns entire cases to train/val/test splits\n",
    "- Ensures all timesteps from one case stay together (prevents data leakage!)\n",
    "- Moves partition files to appropriate subdirectories: `train/`, `val/`, `test/`\n",
    "\n",
    "Output: Organized partition files in split-specific directories\n",
    "\n",
    "##### **Step 4: Compute Global Statistics**\n",
    "\n",
    "```python\n",
    "stats = compute_global_statistics(graph_files, self.stats_file)\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Iterates through all training graphs\n",
    "- Computes mean and standard deviation for:\n",
    "  - Node features (PERMX, PORV, PRESSURE, SWAT, etc.)\n",
    "  - Edge features (transmissibilities)\n",
    "  - Target features (next-timestep values)\n",
    "- Saves statistics to `global_stats.json`\n",
    "\n",
    "Output: `global_stats.json` with normalization parameters\n",
    "\n",
    "**Why Global Statistics?**\n",
    "- Neural networks train better with normalized inputs (typically mean=0, std=1)\n",
    "- Ensures all features are on similar scales\n",
    "- Allows denormalization of predictions for physical interpretation\n",
    "\n",
    "##### **Step 5: Save Metadata**\n",
    "\n",
    "```python\n",
    "self.save_dataset_metadata()\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Records all preprocessing configuration\n",
    "- Saves paths to datasets, partitions, statistics\n",
    "- Stores partition topology (num_partitions, halo_size)\n",
    "- Creates metadata for inference stage\n",
    "\n",
    "Output: `dataset_metadata.json`\n",
    "\n",
    "#### Key Helper Functions\n",
    "\n",
    "**Graph Partitioning with METIS:**\n",
    "```python\n",
    "cluster_data = pyg.loader.ClusterData(\n",
    "    graph, num_parts=num_partitions\n",
    ")\n",
    "```\n",
    "- Uses METIS algorithm to minimize edge cuts between partitions\n",
    "- Balances partition sizes\n",
    "- Falls back to sequential partitioning if METIS unavailable\n",
    "\n",
    "**Halo Region Creation:**\n",
    "```python\n",
    "part_node, part_edge_index, inner_node_mapping, edge_mask = (\n",
    "    pyg.utils.k_hop_subgraph(\n",
    "        part_inner_node,\n",
    "        num_hops=halo_size,  # Usually 1-3\n",
    "        edge_index=graph.edge_index,\n",
    "        num_nodes=graph.num_nodes,\n",
    "        relabel_nodes=True,\n",
    "    )\n",
    ")\n",
    "```\n",
    "- Extends partition by `halo_size` layers of neighboring nodes\n",
    "- Allows information to propagate between partitions during training\n",
    "- Critical for maintaining accuracy with partitioned graphs\n",
    "\n",
    "#### Directory Structure After Preprocessing\n",
    "\n",
    "```\n",
    "outputs/XMGN_2D_Q5SP_Waterflood/\n",
    "├── preprocessed_data/\n",
    "│   ├── graphs/                     # Raw graph files\n",
    "│   │   ├── CASE_001_000.pt\n",
    "│   │   ├── CASE_001_001.pt\n",
    "│   │   └── ...\n",
    "│   ├── partitions/\n",
    "│   │   ├── train/                  # Training partitions\n",
    "│   │   │   ├── partitions_CASE_001_000.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── val/                    # Validation partitions\n",
    "│   │   └── test/                   # Test partitions\n",
    "│   ├── global_stats.json           # Normalization statistics\n",
    "│   └── dataset_metadata.json       # Preprocessing metadata\n",
    "```\n",
    "\n",
    "#### Common Issues and Solutions\n",
    "\n",
    "**Issue**: \"METIS partitioning failed\"\n",
    "- **Solution**: Automatically falls back to simple sequential partitioning\n",
    "- **Impact**: May result in less balanced partitions, but still functional\n",
    "\n",
    "**Issue**: \"Insufficient samples for train/val/test split\"\n",
    "- **Solution**: Need at least 3 simulation cases for splits\n",
    "- **Fix**: Increase `num_samples` in config or adjust split ratios\n",
    "\n",
    "**Issue**: \"NaN in global statistics\"\n",
    "- **Cause**: Invalid or missing data in simulation files\n",
    "- **Fix**: Check simulation outputs for completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69971d23",
   "metadata": {},
   "source": [
    "## Understanding train.py\n",
    "\n",
    "Now let's understand what happens during training. The `train.py` script handles the entire training pipeline, from loading data to saving checkpoints.\n",
    "\n",
    "#### Main Classes and Components\n",
    "\n",
    "**1. `Trainer` - Main training orchestrator class**\n",
    "\n",
    "```python\n",
    "class Trainer:\n",
    "    def __init__(self, cfg, dist, logger):\n",
    "        # Initialize distributed training\n",
    "        self.dist = dist\n",
    "        self.device = dist.device\n",
    "        \n",
    "        # Set up dataloaders\n",
    "        self._initialize_dataloaders(cfg)\n",
    "        \n",
    "        # Create model\n",
    "        self._initialize_model(cfg)\n",
    "        \n",
    "        # Set up optimizer and scheduler\n",
    "        self._initialize_optimizer(cfg)\n",
    "        \n",
    "        # Configure loss functions\n",
    "        self._initialize_loss_functions(cfg)\n",
    "        \n",
    "        # Set up early stopping\n",
    "        self._initialize_early_stopping(cfg)\n",
    "```\n",
    "\n",
    "#### The Training Pipeline\n",
    "\n",
    "##### **Initialization Steps**\n",
    "\n",
    "**1. Initialize Dataloaders** (`_initialize_dataloaders`)\n",
    "\n",
    "```python\n",
    "def _initialize_dataloaders(self, cfg):\n",
    "    # Load global statistics for normalization\n",
    "    self.stats = load_stats(self.stats_file)\n",
    "    \n",
    "    # Create dataset from partition files\n",
    "    dataset = GraphDataset(\n",
    "        file_paths,\n",
    "        node_mean, node_std,\n",
    "        edge_mean, edge_std,\n",
    "        target_mean, target_std\n",
    "    )\n",
    "    \n",
    "    # Create DistributedSampler for multi-GPU\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        sampler=sampler,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Loads partitioned graphs from train/val directories\n",
    "- Applies normalization using statistics from preprocessing\n",
    "- Creates distributed sampler for multi-GPU training\n",
    "- Wraps in DataLoader for batch processing\n",
    "\n",
    "**2. Initialize Model** (`_initialize_model`)\n",
    "\n",
    "```python\n",
    "def _initialize_model(self, cfg):\n",
    "    # Get feature dimensions from statistics\n",
    "    input_dim_nodes = len(self.stats[\"node_features\"][\"mean\"])\n",
    "    input_dim_edges = len(self.stats[\"edge_features\"][\"mean\"])\n",
    "    output_dim = len(cfg.dataset.graph.target_vars.node_features)\n",
    "    \n",
    "    # Create MeshGraphNet model\n",
    "    self.model = MeshGraphNet(\n",
    "        input_dim_nodes=input_dim_nodes,\n",
    "        input_dim_edges=input_dim_edges,\n",
    "        output_dim=output_dim,\n",
    "        processor_size=cfg.model.num_message_passing_layers,\n",
    "        hidden_dim_node_encoder=cfg.model.hidden_dim,\n",
    "        hidden_dim_edge_encoder=cfg.model.hidden_dim,\n",
    "        hidden_dim_node_decoder=cfg.model.hidden_dim,\n",
    "        mlp_activation_fn=cfg.model.activation,\n",
    "        do_concat_trick=cfg.performance.use_concat_trick,\n",
    "        num_processor_checkpoint_segments=cfg.performance.checkpoint_segments,\n",
    "    ).to(self.device)\n",
    "    \n",
    "    # Wrap for distributed training\n",
    "    if world_size > 1:\n",
    "        self.model = DistributedDataParallel(self.model, ...)\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Creates X-MeshGraphNet (MeshGraphNet) architecture\n",
    "- **Encoder**: Maps node/edge features to hidden dimension\n",
    "- **Processor**: Message passing layers (graph convolution)\n",
    "- **Decoder**: Maps back to target variables\n",
    "- Wraps with DistributedDataParallel for multi-GPU training\n",
    "\n",
    "**3. Initialize Optimizer** (`_initialize_optimizer`)\n",
    "\n",
    "```python\n",
    "def _initialize_optimizer(self, cfg):\n",
    "    # AdamW optimizer with weight decay\n",
    "    self.optimizer = optim.AdamW(\n",
    "        self.model.parameters(),\n",
    "        lr=cfg.training.start_lr,\n",
    "        weight_decay=cfg.training.weight_decay,\n",
    "        betas=(0.9, 0.99),\n",
    "        eps=1e-8,\n",
    "    )\n",
    "    \n",
    "    # Cosine annealing learning rate schedule\n",
    "    self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        self.optimizer,\n",
    "        T_max=cfg.training.num_epochs,\n",
    "        eta_min=cfg.training.end_lr\n",
    "    )\n",
    "    \n",
    "    # Gradient scaler for mixed precision\n",
    "    self.scaler = GradScaler() if use_cuda else None\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- **AdamW**: Adaptive learning rate with decoupled weight decay (L2 regularization)\n",
    "- **Cosine Annealing**: Learning rate gradually decreases from `start_lr` to `end_lr`\n",
    "- **GradScaler**: Enables mixed precision training (FP16/BF16) for speed\n",
    "\n",
    "**4. Initialize Loss Functions** (`_initialize_loss_functions`)\n",
    "\n",
    "```python\n",
    "def _initialize_loss_functions(self, cfg):\n",
    "    # Load per-variable loss function configuration\n",
    "    self.loss_functions = cfg.dataset.graph.target_vars.loss_functions\n",
    "    # Example: [\"L2\", \"L1\"] for [PRESSURE, SWAT]\n",
    "    \n",
    "    # Create PyTorch loss objects\n",
    "    self.loss_fn_objects = []\n",
    "    for loss_func in self.loss_functions:\n",
    "        if loss_func == \"L1\":\n",
    "            self.loss_fn_objects.append(torch.nn.L1Loss())\n",
    "        elif loss_func == \"L2\":\n",
    "            self.loss_fn_objects.append(torch.nn.MSELoss())\n",
    "        elif loss_func == \"Huber\":\n",
    "            self.loss_fn_objects.append(\n",
    "                torch.nn.HuberLoss(delta=self.huber_delta)\n",
    "            )\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- Creates separate loss function for each target variable\n",
    "- **L1 (MAE)**: Mean Absolute Error, good for saturation (bounded [0,1])\n",
    "- **L2 (MSE)**: Mean Squared Error, good for pressure (unbounded)\n",
    "- **Huber**: Combines L1 and L2, robust to outliers\n",
    "- Allows weighting: `total_loss = w1 * loss_pressure + w2 * loss_swat`\n",
    "\n",
    "##### **Training Loop** (`train()` method)\n",
    "\n",
    "```python\n",
    "def train(self):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Set epoch for distributed sampler\n",
    "        self.train_sampler.set_epoch(epoch)\n",
    "        \n",
    "        # Training step\n",
    "        train_loss = self.train_epoch()\n",
    "        \n",
    "        # Validation step (every validation_freq epochs)\n",
    "        if epoch % validation_freq == 0:\n",
    "            val_loss, val_denorm_loss, val_metrics = self.validate_epoch()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                save_checkpoint(**self.bst_ckpt_args, epoch=epoch)\n",
    "                best_val_loss = val_loss\n",
    "            \n",
    "            # Check early stopping\n",
    "            if self.early_stopping.should_stop():\n",
    "                break\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if epoch % validation_freq == 0:\n",
    "            save_checkpoint(**self.ckpt_args, epoch=epoch)\n",
    "        \n",
    "        # Update learning rate\n",
    "        self.scheduler.step()\n",
    "```\n",
    "\n",
    "**Single Training Epoch** (`train_epoch()`)\n",
    "\n",
    "```python\n",
    "def train_epoch(self):\n",
    "    self.model.train()  # Set to training mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(self.train_dataloader):\n",
    "        partitions_list, labels = batch\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Process each sample in batch\n",
    "        for partitions in partitions_list:\n",
    "            # Process each partition\n",
    "            for partition in partitions:\n",
    "                # 1. Move data to GPU\n",
    "                partition = partition.to(self.device)\n",
    "                \n",
    "                # 2. Forward pass\n",
    "                pred = self.model(partition.x, partition.edge_attr, partition)\n",
    "                \n",
    "                # 3. Get inner nodes (exclude halo)\n",
    "                pred_inner = pred[partition.inner_node]\n",
    "                target_inner = partition.y[partition.inner_node]\n",
    "                \n",
    "                # 4. Compute loss\n",
    "                loss = self.compute_weighted_loss(pred_inner, target_inner)\n",
    "                loss = loss / (num_partitions * num_samples)\n",
    "                \n",
    "                # 5. Backward pass (accumulate gradients)\n",
    "                if use_cuda:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "        \n",
    "        # 6. Update weights (after all partitions)\n",
    "        if use_cuda:\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "1. **Partition Processing**: Each sample may have multiple partitions\n",
    "2. **Inner Nodes**: Only compute loss on inner nodes (not halo)\n",
    "3. **Gradient Accumulation**: Accumulate gradients across partitions before updating\n",
    "4. **Mixed Precision**: Uses GradScaler for FP16/BF16 training\n",
    "\n",
    "**Single Validation Epoch** (`validate_epoch()`)\n",
    "\n",
    "```python\n",
    "def validate_epoch(self):\n",
    "    self.model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in self.val_dataloader:\n",
    "            partitions_list, labels = batch\n",
    "            \n",
    "            for partitions in partitions_list:\n",
    "                for partition in partitions:\n",
    "                    # Forward pass\n",
    "                    pred = self.model(partition.x, partition.edge_attr, partition)\n",
    "                    pred_inner = pred[partition.inner_node]\n",
    "                    target_inner = partition.y[partition.inner_node]\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = self.compute_weighted_loss(pred_inner, target_inner)\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    # Collect for metrics\n",
    "                    all_predictions.append(pred_inner.cpu().numpy())\n",
    "                    all_targets.append(target_inner.cpu().numpy())\n",
    "    \n",
    "    # Compute per-variable metrics\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    metrics = {}\n",
    "    for i, var_name in enumerate(target_names):\n",
    "        mae = np.mean(np.abs(all_predictions[:, i] - all_targets[:, i]))\n",
    "        rmse = np.sqrt(np.mean((all_predictions[:, i] - all_targets[:, i])**2))\n",
    "        metrics[f\"mae_{var_name}\"] = mae\n",
    "        metrics[f\"rmse_{var_name}\"] = rmse\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "1. **No Gradients**: Disables gradient computation for speed and memory\n",
    "2. **Collect Predictions**: Saves all predictions for detailed metrics\n",
    "3. **Per-Variable Metrics**: Computes MAE and RMSE for each target variable\n",
    "4. **Denormalization**: Can compute metrics in physical units\n",
    "\n",
    "#### Loss Computation\n",
    "\n",
    "**Weighted Multi-Variable Loss:**\n",
    "\n",
    "```python\n",
    "def compute_weighted_loss(self, predictions, targets):\n",
    "    losses_per_var = []\n",
    "    \n",
    "    for i, loss_fn in enumerate(self.loss_fn_objects):\n",
    "        pred_var = predictions[:, i]    # e.g., pressure predictions\n",
    "        target_var = targets[:, i]       # e.g., pressure targets\n",
    "        \n",
    "        # Compute loss for this variable\n",
    "        loss = loss_fn(pred_var, target_var)\n",
    "        losses_per_var.append(loss)\n",
    "    \n",
    "    # Apply weights and sum\n",
    "    losses_tensor = torch.stack(losses_per_var)\n",
    "    weighted_loss = torch.sum(self.target_weights * losses_tensor)\n",
    "    \n",
    "    return weighted_loss\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Config: weights=[1.0, 1.0], loss_functions=[\"L2\", \"L1\"]\n",
    "# PRESSURE: L2 loss with weight 1.0\n",
    "# SWAT: L1 loss with weight 1.0\n",
    "total_loss = 1.0 * MSE(pred_pressure, true_pressure) + \n",
    "             1.0 * MAE(pred_swat, true_swat)\n",
    "```\n",
    "\n",
    "#### Checkpointing\n",
    "\n",
    "**Two Types of Checkpoints:**\n",
    "\n",
    "1. **Regular Checkpoints** (every `validation_freq` epochs)\n",
    "   - Saved to: `checkpoints/`\n",
    "   - Used for resuming training\n",
    "   - Contains: model, optimizer, scheduler, epoch\n",
    "\n",
    "2. **Best Checkpoints** (when validation improves)\n",
    "   - Saved to: `best_checkpoints/`\n",
    "   - Used for inference\n",
    "   - Contains: best model based on validation loss\n",
    "\n",
    "**Checkpoint Contents:**\n",
    "```python\n",
    "{\n",
    "    'epoch': 42,\n",
    "    'model_state_dict': ...,\n",
    "    'optimizer_state_dict': ...,\n",
    "    'scheduler_state_dict': ...,\n",
    "    'best_val_loss': 0.0123,\n",
    "}\n",
    "```\n",
    "\n",
    "#### Distributed Training (Multi-GPU)\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **DistributedDataParallel (DDP)**:\n",
    "   - Replicates model on each GPU\n",
    "   - Each GPU processes different data\n",
    "   - Synchronizes gradients across GPUs\n",
    "\n",
    "2. **DistributedSampler**:\n",
    "   - Splits dataset across GPUs\n",
    "   - Each GPU sees unique samples\n",
    "   - `set_epoch()` ensures different shuffling each epoch\n",
    "\n",
    "3. **Gradient Synchronization**:\n",
    "   - After backward pass, gradients are averaged across GPUs\n",
    "   - Ensures all GPUs update with same gradients\n",
    "\n",
    "#### MLflow Logging\n",
    "\n",
    "**Automatic Logging with PhysicsNeMo:**\n",
    "\n",
    "```python\n",
    "with LaunchLogger(name_space=\"train\", epoch=epoch) as log:\n",
    "    train_loss = self.train_epoch()\n",
    "    log.log_epoch({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"learning_rate\": lr,\n",
    "        \"best_val_loss\": best_val_loss\n",
    "    })\n",
    "```\n",
    "\n",
    "**Logged Metrics:**\n",
    "- Training loss (per epoch)\n",
    "- Validation loss (per validation step)\n",
    "- Learning rate schedule\n",
    "- Per-variable MAE and RMSE\n",
    "- Denormalized metrics (physical units)\n",
    "\n",
    "#### Common Training Issues\n",
    "\n",
    "**Issue**: \"CUDA out of memory\"\n",
    "- **Solutions**:\n",
    "  - Reduce `batch_size`\n",
    "  - Increase `num_partitions`\n",
    "  - Reduce `hidden_dim`\n",
    "  - Enable gradient checkpointing (`checkpoint_segments`)\n",
    "\n",
    "**Issue**: \"Loss becomes NaN\"\n",
    "- **Causes**:\n",
    "  - Learning rate too high\n",
    "  - Invalid normalization statistics\n",
    "  - Numerical instability\n",
    "- **Solutions**:\n",
    "  - Reduce `start_lr`\n",
    "  - Check global_stats.json for NaN values\n",
    "  - Use gradient clipping\n",
    "\n",
    "**Issue**: \"Validation loss not improving\"\n",
    "- **Causes**:\n",
    "  - Overfitting\n",
    "  - Insufficient model capacity\n",
    "  - Learning rate too low\n",
    "- **Solutions**:\n",
    "  - Reduce `weight_decay`\n",
    "  - Increase `hidden_dim` or `num_message_passing_layers`\n",
    "  - Increase `start_lr`\n",
    "  - Add more training data\n",
    "\n",
    "**Issue**: \"Training very slow\"\n",
    "- **Solutions**:\n",
    "  - Use multiple GPUs\n",
    "  - Enable mixed precision training\n",
    "  - Increase `batch_size` if memory allows\n",
    "  - Reduce `num_message_passing_layers`\n",
    "\n",
    "#### Training Output Directory Structure\n",
    "\n",
    "```\n",
    "outputs/XMGN_2D_Q5SP_Waterflood/\n",
    "├── checkpoints/               # Regular checkpoints\n",
    "│   ├── checkpoint.epoch_010.pt\n",
    "│   ├── checkpoint.epoch_020.pt\n",
    "│   └── ...\n",
    "├── best_checkpoints/          # Best model\n",
    "│   └── checkpoint.epoch_042.pt\n",
    "├── mlruns/                    # MLflow tracking\n",
    "│   └── 0/\n",
    "│       └── run_id/\n",
    "│           ├── metrics/\n",
    "│           ├── params/\n",
    "│           └── artifacts/\n",
    "└── logs/                      # Training logs\n",
    "```\n",
    "\n",
    "This training infrastructure provides:\n",
    "- Distributed multi-GPU training\n",
    "- Mixed precision for speed\n",
    "- Flexible loss functions per variable\n",
    "- Comprehensive metrics and logging\n",
    "- Checkpoint management and resume\n",
    "- Early stopping to prevent overtraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd59d68",
   "metadata": {},
   "source": [
    "# Example 2: Norne Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728f221",
   "metadata": {},
   "source": [
    "## Understanding preprocessor.py for Norne \n",
    "\n",
    "The preprocessing for Norne is fundamentally the same as the 2D example, but with important differences due to scale and complexity.\n",
    "\n",
    "#### Key Differences for Norne\n",
    "\n",
    "##### **1. Scale Differences**\n",
    "\n",
    "```python\n",
    "# 2D Example\n",
    "num_nodes ≈ 100-1,000\n",
    "num_edges ≈ 400-4,000\n",
    "graph_size ≈ 10-50 MB\n",
    "\n",
    "# Norne Field\n",
    "num_nodes ≈ 45,000\n",
    "num_edges ≈ 220,000 (including ~5,000 NNCs!)\n",
    "graph_size ≈ 300-500 MB\n",
    "```\n",
    "\n",
    "##### **2. Non-Neighbor Connections (NNCs)**\n",
    "\n",
    "The most important difference: **Faults create NNCs**!\n",
    "\n",
    "```python\n",
    "# In ReservoirGraphBuilder (called by preprocessor):\n",
    "# Regular connections (6-face neighbors in 3D)\n",
    "for each cell:\n",
    "    connect to neighbors: [i-1, i+1, j-1, j+1, k-1, k+1]\n",
    "\n",
    "# PLUS: Non-Neighbor Connections from .INIT file\n",
    "# Example from Norne:\n",
    "NNC connections: [(1234, 5678), (2345, 6789), ...]\n",
    "# Cell 1234 connects to cell 5678 across a fault!\n",
    "```\n",
    "\n",
    "**How NNCs are Handled:**\n",
    "\n",
    "```python\n",
    "# In graph builder:\n",
    "# 1. Read regular grid connections\n",
    "regular_edges = build_6_face_connectivity(grid)\n",
    "\n",
    "# 2. Read NNC data from .INIT file\n",
    "nnc_data = read_nnc_transmissibilities(init_file)\n",
    "# Contains: cell1_index, cell2_index, transmissibility\n",
    "\n",
    "# 3. Add NNCs as additional edges\n",
    "nnc_edges = [(cell1, cell2) for cell1, cell2, trans in nnc_data]\n",
    "all_edges = regular_edges + nnc_edges\n",
    "\n",
    "# 4. Create edge features\n",
    "edge_features = []\n",
    "for edge in all_edges:\n",
    "    if edge in regular_edges:\n",
    "        trans = get_transmissibility(edge, direction)  # TRANX, TRANY, or TRANZ\n",
    "    else:  # NNC\n",
    "        trans = nnc_transmissibilities[edge]  # TRANNNC\n",
    "    edge_features.append(trans)\n",
    "```\n",
    "\n",
    "**Result**: Graph naturally handles faults as edges!\n",
    "\n",
    "##### **3. Multi-Phase Complexity**\n",
    "\n",
    "```python\n",
    "# 2D Example - 2 phases\n",
    "dynamic_features = [\"PRESSURE\", \"SWAT\"]  # Water and oil (implicit)\n",
    "target_vars = [\"PRESSURE\", \"SWAT\"]\n",
    "\n",
    "# Norne - 3 phases\n",
    "dynamic_features = [\"PRESSURE\", \"SOIL\", \"SWAT\", \"SGAS\"]\n",
    "target_vars = [\"PRESSURE\", \"SOIL\", \"SWAT\", \"SGAS\"]\n",
    "```\n",
    "\n",
    "**Feature Dimension Growth:**\n",
    "```python\n",
    "# 2D Example\n",
    "static_features: 5 (PERMX, PORV, X, Y, Z)\n",
    "dynamic_features: 2 variables × 3 timesteps = 6\n",
    "total_node_features: 5 + 6 = 11\n",
    "\n",
    "# Norne\n",
    "static_features: 7 (PERMX, PERMY, PERMZ, PORO, X, Y, Z)\n",
    "dynamic_features: 5 variables × 3 timesteps = 15\n",
    "total_node_features: 7 + 15 = 22\n",
    "```\n",
    "\n",
    "##### **4. Partitioning Strategy**\n",
    "\n",
    "For Norne, partitioning is **essential**, not optional:\n",
    "\n",
    "```python\n",
    "# Recommended config for Norne:\n",
    "preprocessing:\n",
    "  num_partitions: 4-8  # More partitions for large model\n",
    "  halo_size: 3         # Larger halo for better accuracy\n",
    "```\n",
    "\n",
    "**Why More Partitions?**\n",
    "```python\n",
    "# Memory estimate (rough):\n",
    "45,000 nodes × 22 features × 4 bytes = ~4 MB (node features)\n",
    "220,000 edges × 4 features × 4 bytes = ~3.5 MB (edge features)\n",
    "45,000 × 4 targets × 4 bytes = ~0.7 MB (targets)\n",
    "Plus: edge indices, metadata, model activations → ~10-15 MB total\n",
    "\n",
    "# With 4 partitions:\n",
    "Per-partition: ~2.5-4 MB (more manageable)\n",
    "# With halo regions (size=3):\n",
    "Per-partition: ~3-6 MB (includes overlap)\n",
    "```\n",
    "\n",
    "**METIS Partitioning for Norne:**\n",
    "```python\n",
    "# METIS tries to:\n",
    "# 1. Balance partition sizes (each ≈ 45,000/4 = 11,250 nodes)\n",
    "# 2. Minimize edge cuts between partitions\n",
    "# 3. Handle irregular connectivity (faults/NNCs)\n",
    "\n",
    "# Example partition assignment:\n",
    "partition_map = {\n",
    "    0: nodes_in_west_region,    # ~11,000 nodes\n",
    "    1: nodes_in_east_region,    # ~11,500 nodes\n",
    "    2: nodes_in_north_region,   # ~11,200 nodes\n",
    "    3: nodes_in_south_region,   # ~11,300 nodes\n",
    "}\n",
    "\n",
    "# Saved to: CASE_NAME_partitions.json\n",
    "{\n",
    "    \"case_name\": \"NORNE_ATW2013_DOE_0001\",\n",
    "    \"num_partitions\": 4,\n",
    "    \"num_nodes\": 45123,\n",
    "    \"partition_assignment\": [1, 1, 1, 2, 2, ...]  # 1-indexed partition ID per node\n",
    "}\n",
    "```\n",
    "\n",
    "##### **5. Processing Time**\n",
    "\n",
    "```\n",
    "2D Example: 1000 cases × 0.1s/case = ~2 minutes\n",
    "Norne: 500 cases × 10s/case = ~80 minutes\n",
    "\n",
    "Why slower?\n",
    "- 45x more nodes to process\n",
    "- Reading larger binary files (200MB vs 5MB per case)\n",
    "- More complex graph operations\n",
    "- Larger data to write to disk\n",
    "```\n",
    "\n",
    "#### The Preprocessing Pipeline for Norne\n",
    "\n",
    "The **same 5 steps** as 2D, but with modifications:\n",
    "\n",
    "##### **Step 1: Create Raw Graphs**\n",
    "\n",
    "```python\n",
    "# ReservoirGraphBuilder for Norne:\n",
    "for case in cases:\n",
    "    # 1. Read .EGRID (geometry)\n",
    "    grid = read_eclipse_grid(f\"{case}.EGRID\")\n",
    "    # → 46×112×22 = 113,344 total cells\n",
    "    # → ~45,000 active cells\n",
    "    # → Corner-point geometry (irregular hexahedra)\n",
    "    \n",
    "    # 2. Read .INIT (static properties)\n",
    "    init_data = read_eclipse_init(f\"{case}.INIT\")\n",
    "    # → PERMX, PERMY, PERMZ, PORO\n",
    "    # → TRANX, TRANY, TRANZ\n",
    "    # → NNC transmissibilities (TRANNNC)\n",
    "    \n",
    "    # 3. Read .UNRST (dynamic properties)\n",
    "    restart_data = read_eclipse_restart(f\"{case}.UNRST\")\n",
    "    # → 64 timesteps\n",
    "    # → PRESSURE, SOIL, SWAT, SGAS per timestep\n",
    "    # → ~2.8 million values per case!\n",
    "    \n",
    "    # 4. Build graph\n",
    "    graph = build_graph(\n",
    "        nodes=active_cells,           # 45,000\n",
    "        regular_edges=6_face_conn,    # ~215,000\n",
    "        nnc_edges=fault_connections,  # ~5,000\n",
    "        node_features=static_props,   # PERM*, PORO, coords\n",
    "        edge_features=trans_values,   # TRAN*, TRANNNC\n",
    "        dynamic_features=timestep_data # PRESSURE, S*\n",
    "    )\n",
    "    \n",
    "    # 5. Create training sequences\n",
    "    for t in range(2, 63):  # 64 timesteps → 62 training samples\n",
    "        input_graph = create_input(\n",
    "            static=static_props,\n",
    "            dynamic_t0=restart_data[t-2],\n",
    "            dynamic_t1=restart_data[t-1],\n",
    "            dynamic_t2=restart_data[t]\n",
    "        )\n",
    "        target = restart_data[t+1]\n",
    "        \n",
    "        save_graph(f\"{case}_{t:03d}.pt\", input_graph, target)\n",
    "```\n",
    "\n",
    "Output: `500 cases × 62 timesteps = 31,000 graph files` (~15 GB total)\n",
    "\n",
    "##### **Step 2: Create Partitions**\n",
    "\n",
    "```python\n",
    "# For each of 31,000 graphs:\n",
    "graph = torch.load(f\"NORNE_case_{timestep}.pt\")\n",
    "\n",
    "# Use METIS to partition into 4 subgraphs\n",
    "cluster_data = pyg.loader.ClusterData(graph, num_parts=4)\n",
    "\n",
    "# For each partition:\n",
    "for part_idx in range(4):\n",
    "    # Get inner nodes (actual partition)\n",
    "    inner_nodes = cluster_data.partition.node_perm[\n",
    "        cluster_data.partition.partptr[part_idx]:\n",
    "        cluster_data.partition.partptr[part_idx + 1]\n",
    "    ]\n",
    "    # → ~11,250 nodes per partition\n",
    "    \n",
    "    # Add halo region (3-hop neighbors)\n",
    "    part_nodes, part_edges, inner_mapping, edge_mask = (\n",
    "        pyg.utils.k_hop_subgraph(\n",
    "            inner_nodes,\n",
    "            num_hops=3,  # halo_size\n",
    "            edge_index=graph.edge_index,\n",
    "            num_nodes=graph.num_nodes\n",
    "        )\n",
    "    )\n",
    "    # → ~13,000-14,000 nodes with halo (includes overlap)\n",
    "    \n",
    "    # Extract partition data\n",
    "    partition = Data(\n",
    "        x=graph.x[part_nodes],              # Node features\n",
    "        edge_index=part_edges,               # Connectivity\n",
    "        edge_attr=graph.edge_attr[edge_mask], # Edge features\n",
    "        y=graph.y[part_nodes],               # Targets\n",
    "        inner_node=inner_mapping,            # Which nodes are \"real\"\n",
    "        part_node=part_nodes                 # Original node indices\n",
    "    )\n",
    "    \n",
    "    partitions.append(partition)\n",
    "\n",
    "# Save all 4 partitions together\n",
    "torch.save(partitions, f\"partitions_{case}_{timestep}.pt\")\n",
    "```\n",
    "\n",
    "Output: 31,000 partition files (~200 GB total with redundancy from halos)\n",
    "\n",
    "##### **Step 3: Split Samples by Case**\n",
    "\n",
    "```python\n",
    "# Norne specific:\n",
    "# 500 cases → \n",
    "#   400 training (80%)\n",
    "#   50 validation (10%)\n",
    "#   50 testing (10%)\n",
    "\n",
    "# Each case has 62 timesteps\n",
    "# → Training: 400 × 62 = 24,800 graphs\n",
    "# → Validation: 50 × 62 = 3,100 graphs\n",
    "# → Testing: 50 × 62 = 3,100 graphs\n",
    "```\n",
    "\n",
    "##### **Step 4: Compute Global Statistics**\n",
    "\n",
    "```python\n",
    "# Critical for Norne due to heterogeneity:\n",
    "\n",
    "# PERMX statistics:\n",
    "mean_permx = 145.3 mD (millidarcies)\n",
    "std_permx = 892.4 mD\n",
    "min_permx = 0.01 mD\n",
    "max_permx = 8500 mD\n",
    "\n",
    "# → Spans 6 orders of magnitude!\n",
    "# → Log-transform essential: PERMX:LOG10\n",
    "\n",
    "# After log-transform:\n",
    "log_permx_mean = 1.84 (log10 mD)\n",
    "log_permx_std = 0.95\n",
    "# → Much more normalized distribution\n",
    "```\n",
    "\n",
    "**Why Log-Transform Matters:**\n",
    "\n",
    "```python\n",
    "# Without log:\n",
    "normalized_perm = (perm - mean) / std\n",
    "# Problem: Most values near zero, outliers dominate\n",
    "# Network struggles to learn\n",
    "\n",
    "# With log:\n",
    "log_perm = np.log10(perm + epsilon)  # epsilon=1e-10 to avoid log(0)\n",
    "normalized_log_perm = (log_perm - log_mean) / log_std\n",
    "# Benefit: More uniform distribution, easier to learn\n",
    "```\n",
    "\n",
    "#### Norne-Specific Preprocessing Challenges\n",
    "\n",
    "##### **Challenge 1: Inactive Cells**\n",
    "\n",
    "```python\n",
    "# Total grid cells: 46 × 112 × 22 = 113,344\n",
    "# Active cells (in reservoir): ~45,000\n",
    "# Inactive cells (outside reservoir): ~68,000\n",
    "\n",
    "# How handled:\n",
    "# 1. EGRID file contains ACTNUM array\n",
    "ACTNUM = [1, 1, 0, 0, 1, ...]  # 1=active, 0=inactive\n",
    "\n",
    "# 2. Only process active cells\n",
    "active_indices = np.where(ACTNUM == 1)[0]\n",
    "nodes = cells[active_indices]\n",
    "\n",
    "# 3. Renumber for graph\n",
    "# Global index → Graph index mapping\n",
    "global_to_graph = {global_idx: graph_idx \n",
    "                   for graph_idx, global_idx in enumerate(active_indices)}\n",
    "```\n",
    "\n",
    "##### **Challenge 2: Memory Management**\n",
    "\n",
    "```python\n",
    "# Strategies for handling large graphs:\n",
    "\n",
    "# 1. Process in chunks\n",
    "for case_batch in chunks(cases, chunk_size=10):\n",
    "    process_batch(case_batch)\n",
    "    torch.cuda.empty_cache()  # Free GPU memory\n",
    "\n",
    "# 2. Use memory-efficient dtypes\n",
    "node_features = torch.tensor(features, dtype=torch.float32)  # Not float64\n",
    "edge_indices = torch.tensor(edges, dtype=torch.long)          # Not int64\n",
    "\n",
    "# 3. Stream to disk immediately\n",
    "graph = create_graph(...)\n",
    "torch.save(graph, filename)\n",
    "del graph  # Free memory immediately\n",
    "```\n",
    "\n",
    "##### **Challenge 3: NNC Edge Ordering**\n",
    "\n",
    "```python\n",
    "# NNCs don't follow regular ordering\n",
    "# Regular edges: predictable (i, j, k) → (i+1, j, k)\n",
    "# NNCs: arbitrary (1234) → (5678)\n",
    "\n",
    "# Solution: Store edge type\n",
    "edge_type = []\n",
    "for edge in edges:\n",
    "    if edge in regular_edges:\n",
    "        if edge[1] - edge[0] == 1:\n",
    "            edge_type.append(0)  # X-direction\n",
    "        elif edge[1] - edge[0] == nx:\n",
    "            edge_type.append(1)  # Y-direction\n",
    "        elif edge[1] - edge[0] == nx * ny:\n",
    "            edge_type.append(2)  # Z-direction\n",
    "    else:\n",
    "        edge_type.append(3)  # NNC (fault)\n",
    "\n",
    "# Helps model learn directional vs fault connections differently\n",
    "```\n",
    "\n",
    "#### Output Directory Structure for Norne\n",
    "\n",
    "```\n",
    "outputs/XMGN_Norne_Field/\n",
    "├── preprocessed_data/\n",
    "│   ├── graphs/                          # Raw graphs\n",
    "│   │   ├── NORNE_ATW2013_DOE_0001_000.pt\n",
    "│   │   ├── NORNE_ATW2013_DOE_0001_001.pt\n",
    "│   │   └── ... (31,000 files, ~15 GB)\n",
    "│   │\n",
    "│   ├── partitions/\n",
    "│   │   ├── train/                       # 24,800 files\n",
    "│   │   │   ├── partitions_NORNE_..._000.pt\n",
    "│   │   │   └── ...\n",
    "│   │   ├── val/                         # 3,100 files\n",
    "│   │   └── test/                        # 3,100 files\n",
    "│   │\n",
    "│   ├── global_stats.json                # Normalization stats\n",
    "│   ├── dataset_metadata.json            # Metadata\n",
    "│   │\n",
    "│   └── NORNE_ATW2013_DOE_*_partitions.json  # Partition assignments (500 files)\n",
    "│       # For visualization in ResInsight\n",
    "```\n",
    "\n",
    "#### Preprocessing Time Estimates\n",
    "\n",
    "```\n",
    "Step 1: Create raw graphs\n",
    "- 500 cases × 10s/case = ~83 minutes\n",
    "- Bottleneck: Reading large binary files\n",
    "\n",
    "Step 2: Create partitions  \n",
    "- 31,000 graphs × 3s/graph = ~26 hours\n",
    "- Bottleneck: METIS partitioning\n",
    "\n",
    "Step 3: Split and organize\n",
    "- ~5 minutes (mostly file operations)\n",
    "\n",
    "Step 4: Compute statistics\n",
    "- ~30 minutes (iterate through all training graphs)\n",
    "\n",
    "Total: ~28-30 hours for full preprocessing\n",
    "```\n",
    "\n",
    "**Optimization Tips:**\n",
    "```python\n",
    "# Use multiple workers:\n",
    "num_preprocess_workers: 8  # Process 8 cases in parallel\n",
    "\n",
    "# Estimated speedup: ~4-5x\n",
    "# New total time: ~6-8 hours\n",
    "```\n",
    "\n",
    "#### Verification After Preprocessing\n",
    "\n",
    "```python\n",
    "# Check key outputs:\n",
    "print(\"Graphs created:\", len(os.listdir(\"graphs/\")))\n",
    "# Expected: 31,000\n",
    "\n",
    "print(\"Partitions created:\", len(os.listdir(\"partitions/train/\")))\n",
    "# Expected: 24,800\n",
    "\n",
    "# Check statistics\n",
    "with open(\"global_stats.json\") as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"Node features:\", len(stats[\"node_features\"][\"mean\"]))\n",
    "# Expected: 22 (7 static + 5 dynamic × 3 timesteps)\n",
    "\n",
    "print(\"Edge features:\", len(stats[\"edge_features\"][\"mean\"]))\n",
    "# Expected: 4 (TRANX, TRANY, TRANZ, TRANNNC)\n",
    "\n",
    "print(\"Target features:\", len(stats[\"target_features\"][\"mean\"]))\n",
    "# Expected: 4 (PRESSURE, SOIL, SWAT, SGAS)\n",
    "\n",
    "# Check for NaN in statistics\n",
    "assert not any(np.isnan(stats[\"node_features\"][\"mean\"]))\n",
    "assert not any(np.isnan(stats[\"edge_features\"][\"mean\"]))\n",
    "```\n",
    "\n",
    "This preprocessing pipeline transforms 500 complex 3D reservoir simulations with faults into ~31,000 graph structures ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1d7f9",
   "metadata": {},
   "source": [
    "## Understanding train.py for Norne \n",
    "\n",
    "Training for Norne uses the **same training script** as the 2D example, but the scale and complexity require different strategies and configurations.\n",
    "\n",
    "#### Key Differences for Norne Training\n",
    "\n",
    "##### **1. Model Capacity Requirements**\n",
    "\n",
    "```python\n",
    "# 2D Example Configuration\n",
    "model:\n",
    "  num_message_passing_layers: 3\n",
    "  hidden_dim: 64\n",
    "  \n",
    "# Typical model size: ~500K parameters\n",
    "\n",
    "# Norne Configuration  \n",
    "model:\n",
    "  num_message_passing_layers: 4  # Deeper for complex patterns\n",
    "  hidden_dim: 128                 # Wider for more features\n",
    "  \n",
    "# Typical model size: ~2-3M parameters\n",
    "\n",
    "# Why more capacity needed?\n",
    "# - More input features (22 vs 11)\n",
    "# - More output features (4 vs 2)\n",
    "# - Longer-range interactions (3D geometry)\n",
    "# - More complex physics (3-phase, faults)\n",
    "```\n",
    "\n",
    "##### **2. Memory and Batch Size**\n",
    "\n",
    "```python\n",
    "# Memory requirements per sample:\n",
    "\n",
    "# 2D Example (1 partition):\n",
    "graph_size = 1,000 nodes × 11 features = ~44 KB\n",
    "model_activations = ~5 MB\n",
    "total_per_sample = ~5-10 MB\n",
    "\n",
    "# Norne (4 partitions with halos):\n",
    "partition_size = 13,000 nodes × 22 features = ~1.1 MB per partition\n",
    "4 partitions = ~4.5 MB\n",
    "model_activations = ~100-200 MB per partition\n",
    "total_per_sample = ~400-800 MB!\n",
    "\n",
    "# Consequence:\n",
    "training:\n",
    "  batch_size: 1  # Must use batch_size=1 for Norne!\n",
    "```\n",
    "\n",
    "**Why batch_size=1 Works:**\n",
    "```python\n",
    "# Even with batch_size=1, we have:\n",
    "# - 4 partitions per sample\n",
    "# - Multiple GPUs processing in parallel\n",
    "# → Effective parallelism maintained\n",
    "\n",
    "# Example with 4 GPUs:\n",
    "# GPU 0: processes partition 0 of sample\n",
    "# GPU 1: processes partition 1 of sample\n",
    "# GPU 2: processes partition 2 of sample\n",
    "# GPU 3: processes partition 3 of sample\n",
    "# → All GPUs busy, efficient utilization\n",
    "```\n",
    "\n",
    "##### **3. Training Time Estimates**\n",
    "\n",
    "```\n",
    "Component                2D Example      Norne Field\n",
    "─────────────────────────────────────────────────────\n",
    "Forward pass (1 sample)   0.1s            2-5s\n",
    "Backward pass (1 sample)  0.2s            5-10s\n",
    "Total per sample          0.3s            7-15s\n",
    "─────────────────────────────────────────────────────\n",
    "Samples per epoch         800             24,800\n",
    "Time per epoch (1 GPU)    4 min           ~70 hours (!)\n",
    "Time per epoch (4 GPUs)   2 min           ~20 hours\n",
    "Time per epoch (8 GPUs)   1 min           ~10 hours\n",
    "─────────────────────────────────────────────────────\n",
    "Target epochs             1000            500\n",
    "Total training (1 GPU)    ~67 hours       ~14,600 hours (!!)\n",
    "Total training (4 GPUs)   ~33 hours       ~4,200 hours (!!)\n",
    "Total training (8 GPUs)   ~17 hours       ~2,100 hours (~87 days)\n",
    "```\n",
    "\n",
    "**Actual Training Time (with early stopping):**\n",
    "```\n",
    "# Typically converges in 100-200 epochs for Norne\n",
    "# With 8 GPUs:\n",
    "100 epochs × 10 hours/epoch = ~1,000 hours = ~42 days\n",
    "# With early stopping patience=30:\n",
    "# Realistic: 150 epochs = ~63 hours = 2.6 days\n",
    "```\n",
    "\n",
    "##### **4. Multi-GPU Training Strategy**\n",
    "\n",
    "**Distributed Data Parallel (DDP) for Norne:**\n",
    "\n",
    "```python\n",
    "# Launch training with 4 GPUs:\n",
    "# torchrun --nproc_per_node=4 src/train.py --config-name=config_norne\n",
    "\n",
    "# What happens:\n",
    "# 1. Model replicated on each GPU\n",
    "Rank 0 (GPU 0): model_copy_0\n",
    "Rank 1 (GPU 1): model_copy_1\n",
    "Rank 2 (GPU 2): model_copy_2\n",
    "Rank 3 (GPU 3): model_copy_3\n",
    "\n",
    "# 2. Data distributed across GPUs\n",
    "DistributedSampler splits 24,800 samples:\n",
    "Rank 0: samples [0, 4, 8, 12, ...]      # 6,200 samples\n",
    "Rank 1: samples [1, 5, 9, 13, ...]      # 6,200 samples\n",
    "Rank 2: samples [2, 6, 10, 14, ...]     # 6,200 samples\n",
    "Rank 3: samples [3, 7, 11, 15, ...]     # 6,200 samples\n",
    "\n",
    "# 3. Forward pass (parallel)\n",
    "Each GPU: processes its assigned samples independently\n",
    "\n",
    "# 4. Backward pass (synchronized)\n",
    "Each GPU: computes gradients for its samples\n",
    "All GPUs: average gradients via AllReduce\n",
    "Result: synchronized gradients on all GPUs\n",
    "\n",
    "# 5. Optimizer step (synchronized)\n",
    "Each GPU: updates model with averaged gradients\n",
    "Result: all model copies stay in sync\n",
    "```\n",
    "\n",
    "**Communication Pattern:**\n",
    "\n",
    "```python\n",
    "# Every training step:\n",
    "# 1. Forward pass: No communication (independent)\n",
    "# 2. Loss computation: No communication\n",
    "# 3. Backward pass: AllReduce of gradients\n",
    "#    - Size: ~2-3M parameters × 4 bytes = ~8-12 MB\n",
    "#    - Time: ~10-50ms depending on interconnect\n",
    "# 4. Optimizer step: No communication (uses synced gradients)\n",
    "\n",
    "# Key insight: Communication overhead is small compared to computation!\n",
    "# Computation: 5-10 seconds\n",
    "# Communication: 0.01-0.05 seconds\n",
    "# Efficiency: >99%\n",
    "```\n",
    "\n",
    "#### Norne-Specific Training Configurations\n",
    "\n",
    "##### **Learning Rate Schedule**\n",
    "\n",
    "```python\n",
    "# For Norne, use more conservative learning rates:\n",
    "\n",
    "training:\n",
    "  num_epochs: 500\n",
    "  start_lr: 5e-5   # Lower than 2D (1e-4)\n",
    "  end_lr: 1e-6     # Same as 2D\n",
    "  weight_decay: 1e-3\n",
    "\n",
    "# Why lower start_lr?\n",
    "# - More complex model (easier to destabilize)\n",
    "# - More features (higher dimensional space)\n",
    "# - More parameters (larger gradient magnitudes)\n",
    "\n",
    "# Cosine annealing schedule:\n",
    "epoch  0: lr = 5e-5\n",
    "epoch 50: lr = 4.3e-5\n",
    "epoch 100: lr = 3.2e-5\n",
    "epoch 250: lr = 1.3e-5\n",
    "epoch 400: lr = 1.1e-6\n",
    "epoch 500: lr = 1e-6\n",
    "```\n",
    "\n",
    "##### **Loss Function Configuration**\n",
    "\n",
    "```python\n",
    "# Norne has 4 target variables, each with its own loss:\n",
    "\n",
    "dataset:\n",
    "  graph:\n",
    "    target_vars:\n",
    "      node_features: [\"PRESSURE\", \"SOIL\", \"SWAT\", \"SGAS\"]\n",
    "      weights: [1.0, 1.0, 1.0, 1.0]  # Equal weighting\n",
    "      loss_functions: [\"L2\", \"L1\", \"L1\", \"L1\"]\n",
    "\n",
    "# Why this choice?\n",
    "# - PRESSURE: L2 (MSE) - continuous, wide range\n",
    "# - SOIL: L1 (MAE) - bounded [0,1], sparse in some regions\n",
    "# - SWAT: L1 (MAE) - bounded [0,1]\n",
    "# - SGAS: L1 (MAE) - bounded [0,1], often very small values\n",
    "\n",
    "# Total loss computation:\n",
    "total_loss = (\n",
    "    1.0 × MSE(pred_pressure, true_pressure) +\n",
    "    1.0 × MAE(pred_soil, true_soil) +\n",
    "    1.0 × MAE(pred_swat, true_swat) +\n",
    "    1.0 × MAE(pred_sgas, true_sgas)\n",
    ")\n",
    "```\n",
    "\n",
    "**Loss Weighting Strategy:**\n",
    "\n",
    "You might want to adjust weights based on variable importance:\n",
    "\n",
    "```python\n",
    "# Option 1: Equal weights (default)\n",
    "weights: [1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "# Option 2: Emphasize pressure (most important for production)\n",
    "weights: [2.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "# Option 3: Normalize by typical ranges\n",
    "# PRESSURE: 100-400 bar → range ~ 300\n",
    "# SOIL: 0-1 → range ~ 1\n",
    "# SWAT: 0-1 → range ~ 1\n",
    "# SGAS: 0-0.5 → range ~ 0.5\n",
    "weights: [0.01, 1.0, 1.0, 2.0]  # Inverse of ranges\n",
    "```\n",
    "\n",
    "##### **Early Stopping Configuration**\n",
    "\n",
    "```python\n",
    "# Norne requires more patience:\n",
    "\n",
    "training:\n",
    "  early_stopping:\n",
    "    patience: 30  # vs 20 for 2D\n",
    "    min_delta: 1e-6\n",
    "\n",
    "# Why more patience?\n",
    "# - Larger model takes longer to converge\n",
    "# - More data means more epochs to see all samples\n",
    "# - Validation loss may plateau then improve\n",
    "\n",
    "# Example training curve:\n",
    "Epoch 10: val_loss = 0.0156\n",
    "Epoch 20: val_loss = 0.0121\n",
    "Epoch 30: val_loss = 0.0098\n",
    "Epoch 40: val_loss = 0.0092  # Plateau starts\n",
    "Epoch 50: val_loss = 0.0089\n",
    "Epoch 60: val_loss = 0.0087\n",
    "Epoch 70: val_loss = 0.0086  # Still slowly improving\n",
    "Epoch 80: val_loss = 0.0081  # Breaks through!\n",
    "# Without patience=30, would have stopped too early\n",
    "```\n",
    "\n",
    "##### **Validation Frequency**\n",
    "\n",
    "```python\n",
    "# Balance between monitoring and training time:\n",
    "\n",
    "training:\n",
    "  validation_freq: 10  # Validate every 10 epochs\n",
    "\n",
    "# Time impact:\n",
    "# - Training epoch: 10 hours\n",
    "# - Validation: 2 hours (50 validation samples)\n",
    "# - Total per validation: 12 hours\n",
    "\n",
    "# Alternatives:\n",
    "validation_freq: 5   # More frequent, slower training\n",
    "validation_freq: 20  # Less frequent, faster but less monitoring\n",
    "```\n",
    "\n",
    "#### Memory Optimization Techniques for Norne\n",
    "\n",
    "##### **1. Gradient Checkpointing**\n",
    "\n",
    "```python\n",
    "performance:\n",
    "  checkpoint_segments: 4  # More segments = less memory\n",
    "\n",
    "# How it works:\n",
    "# Normal: Store all activations during forward pass\n",
    "# Memory: num_layers × hidden_dim × num_nodes = large!\n",
    "\n",
    "# With checkpointing:\n",
    "# - Divide model into 4 segments\n",
    "# - Only store activations at segment boundaries\n",
    "# - Recompute intermediate activations during backward\n",
    "\n",
    "# Trade-off:\n",
    "# Memory: Reduced by ~50-70%\n",
    "# Speed: Slowed by ~10-20% (recomputation overhead)\n",
    "# → Worth it for large models!\n",
    "```\n",
    "\n",
    "##### **2. Mixed Precision Training**\n",
    "\n",
    "```python\n",
    "# Automatically enabled for CUDA:\n",
    "self.scaler = GradScaler()\n",
    "\n",
    "# Forward pass in FP16/BF16:\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    pred = self.model(x, edge_attr, graph)\n",
    "\n",
    "# Backward pass with scaling:\n",
    "self.scaler.scale(loss).backward()\n",
    "self.scaler.step(optimizer)\n",
    "self.scaler.update()\n",
    "\n",
    "# Benefits for Norne:\n",
    "# - Memory: 2x reduction (FP16 vs FP32)\n",
    "# - Speed: 1.5-2x faster (tensor cores)\n",
    "# - Accuracy: Minimal impact with proper scaling\n",
    "\n",
    "# Note: BF16 often better than FP16 for reservoir sim\n",
    "# (wider dynamic range handles pressure variations better)\n",
    "```\n",
    "\n",
    "##### **3. Concatenation Trick**\n",
    "\n",
    "```python\n",
    "performance:\n",
    "  use_concat_trick: true\n",
    "\n",
    "# Standard MeshGraphNet:\n",
    "# node_update = MLP(node_feat)\n",
    "# edge_update = MLP(edge_feat)\n",
    "# Requires: 2 separate MLP passes\n",
    "\n",
    "# With concat trick:\n",
    "# combined = concat([node_feat, edge_feat])\n",
    "# updates = MLP(combined)  # Single pass!\n",
    "# node_update, edge_update = split(updates)\n",
    "\n",
    "# Benefits:\n",
    "# - Fewer MLP calls\n",
    "# - Better GPU utilization\n",
    "# - ~10-15% speedup\n",
    "```\n",
    "\n",
    "#### Monitoring Training for Norne\n",
    "\n",
    "##### **Key Metrics to Watch**\n",
    "\n",
    "```python\n",
    "# 1. Training Loss\n",
    "# Should decrease smoothly\n",
    "epoch 10: train_loss = 0.0234\n",
    "epoch 20: train_loss = 0.0178\n",
    "epoch 30: train_loss = 0.0145\n",
    "# Good: Steady decrease\n",
    "# Bad: Oscillating or NaN\n",
    "\n",
    "# 2. Validation Loss  \n",
    "# Should track training loss\n",
    "epoch 10: val_loss = 0.0241 (train: 0.0234) → gap: 0.0007\n",
    "epoch 20: val_loss = 0.0185 (train: 0.0178) → gap: 0.0007\n",
    "# Good: Small gap (< 10%)\n",
    "# Bad: Growing gap (overfitting)\n",
    "\n",
    "# 3. Per-Variable Metrics (Denormalized)\n",
    "# PRESSURE RMSE: 5-15 bar (acceptable for 100-400 bar range)\n",
    "# SOIL MAE: 0.05-0.10 (5-10% error)\n",
    "# SWAT MAE: 0.05-0.10\n",
    "# SGAS MAE: 0.02-0.05\n",
    "\n",
    "# 4. Learning Rate\n",
    "# Should follow cosine schedule\n",
    "epoch 0: lr = 5e-5\n",
    "epoch 250: lr = 1.3e-5\n",
    "epoch 500: lr = 1e-6\n",
    "```\n",
    "\n",
    "##### **MLflow Metrics Logged**\n",
    "\n",
    "```python\n",
    "# Training metrics (every epoch):\n",
    "- train_loss\n",
    "- learning_rate\n",
    "- best_val_loss\n",
    "\n",
    "# Validation metrics (every validation_freq epochs):\n",
    "- val_loss\n",
    "- val_denorm_loss\n",
    "\n",
    "# Per-variable metrics (normalized):\n",
    "- val_mae_pressure, val_rmse_pressure\n",
    "- val_mae_soil, val_rmse_soil\n",
    "- val_mae_swat, val_rmse_swat\n",
    "- val_mae_sgas, val_rmse_sgas\n",
    "\n",
    "# Per-variable metrics (denormalized, physical units):\n",
    "- val_mae_pressure_denorm, val_rmse_pressure_denorm\n",
    "- val_mae_soil_denorm, val_rmse_soil_denorm\n",
    "- val_mae_swat_denorm, val_rmse_swat_denorm\n",
    "- val_mae_sgas_denorm, val_rmse_sgas_denorm\n",
    "```\n",
    "\n",
    "#### Common Training Issues for Norne\n",
    "\n",
    "##### **Issue 1: CUDA Out of Memory**\n",
    "\n",
    "```python\n",
    "# Error: RuntimeError: CUDA out of memory. Tried to allocate X GB\n",
    "\n",
    "# Solutions (in order of preference):\n",
    "# 1. Increase num_partitions\n",
    "num_partitions: 8  # vs 4 (halves memory per partition)\n",
    "\n",
    "# 2. Increase checkpoint_segments\n",
    "checkpoint_segments: 6  # vs 4\n",
    "\n",
    "# 3. Reduce hidden_dim\n",
    "hidden_dim: 96  # vs 128 (reduces model size)\n",
    "\n",
    "# 4. Reduce batch_size (if > 1)\n",
    "batch_size: 1  # Already at minimum for Norne\n",
    "\n",
    "# 5. Use fewer GPUs (counterintuitive but works)\n",
    "# Sometimes 4 GPUs with more memory > 8 GPUs with less memory\n",
    "```\n",
    "\n",
    "##### **Issue 2: Slow Training**\n",
    "\n",
    "```python\n",
    "# If training is slower than expected:\n",
    "\n",
    "# Check 1: GPU utilization\n",
    "nvidia-smi\n",
    "# Should show ~95%+ GPU utilization\n",
    "# If low: bottleneck is data loading or CPU\n",
    "\n",
    "# Solution for data loading:\n",
    "num_preprocess_workers: 8  # More workers\n",
    "pin_memory: true           # Faster CPU→GPU transfer\n",
    "\n",
    "# Check 2: Multi-GPU efficiency\n",
    "# With 4 GPUs, expect ~3.5x speedup (not 4x due to overhead)\n",
    "# If < 3x: Communication bottleneck\n",
    "\n",
    "# Solution:\n",
    "# Use faster interconnect (NVLink > PCIe)\n",
    "# Or reduce num_partitions (less communication)\n",
    "\n",
    "# Check 3: Gradient checkpointing overhead\n",
    "checkpoint_segments: 2  # Reduce if memory allows\n",
    "# Fewer segments = less recomputation = faster\n",
    "```\n",
    "\n",
    "##### **Issue 3: Validation Loss Plateau**\n",
    "\n",
    "```python\n",
    "# Validation loss stops improving:\n",
    "\n",
    "# Cause 1: Learning rate too low\n",
    "# Solution: Increase start_lr\n",
    "start_lr: 1e-4  # vs 5e-5\n",
    "\n",
    "# Cause 2: Model capacity insufficient\n",
    "# Solution: Increase model size\n",
    "hidden_dim: 192  # vs 128\n",
    "num_message_passing_layers: 5  # vs 4\n",
    "\n",
    "# Cause 3: Overfitting\n",
    "# Solution: Increase regularization\n",
    "weight_decay: 5e-3  # vs 1e-3\n",
    "\n",
    "# Cause 4: Not enough training data\n",
    "# Solution: Add more simulation cases\n",
    "```\n",
    "\n",
    "##### **Issue 4: NaN Loss**\n",
    "\n",
    "```python\n",
    "# Loss becomes NaN during training\n",
    "\n",
    "# Cause 1: Gradient explosion\n",
    "# Check: Large gradients\n",
    "for param in model.parameters():\n",
    "    print(param.grad.max())  # If > 100: problem!\n",
    "\n",
    "# Solution: Gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# Cause 2: Invalid normalization statistics\n",
    "# Check: global_stats.json for NaN or inf values\n",
    "\n",
    "# Solution: Rerun preprocessing with data validation\n",
    "\n",
    "# Cause 3: Learning rate too high\n",
    "# Solution: Reduce start_lr\n",
    "start_lr: 1e-5  # Very conservative\n",
    "\n",
    "# Cause 4: Numerical instability in model\n",
    "# Solution: Use BF16 instead of FP16 (wider dynamic range)\n",
    "```\n",
    "\n",
    "#### Checkpoint Management for Norne\n",
    "\n",
    "```python\n",
    "# With long training times, checkpoint management is critical:\n",
    "\n",
    "# Regular checkpoints (every 10 epochs):\n",
    "checkpoints/\n",
    "├── checkpoint.epoch_010.pt  # ~50 MB\n",
    "├── checkpoint.epoch_020.pt\n",
    "├── checkpoint.epoch_030.pt\n",
    "└── ...\n",
    "\n",
    "# Best checkpoint (validation improvements):\n",
    "best_checkpoints/\n",
    "└── checkpoint.epoch_087.pt  # Best val_loss\n",
    "\n",
    "# Checkpoint cleanup strategy:\n",
    "# - Keep last 3 regular checkpoints (in case of corruption)\n",
    "# - Keep all best checkpoints (for analysis)\n",
    "# - Delete old regular checkpoints\n",
    "\n",
    "# Resume training:\n",
    "training:\n",
    "  resume: true  # Automatically loads latest checkpoint\n",
    "\n",
    "# Load specific checkpoint for inference:\n",
    "inference:\n",
    "  checkpoint_path: \"outputs/.../best_checkpoints/checkpoint.epoch_087.pt\"\n",
    "```\n",
    "\n",
    "#### Training Monitoring Dashboard\n",
    "\n",
    "```bash\n",
    "# Launch MLflow UI (in separate terminal):\n",
    "cd outputs/XMGN_Norne_Field\n",
    "mlflow ui --host 0.0.0.0 --port 5000\n",
    "\n",
    "# Open browser: http://localhost:5000\n",
    "\n",
    "# What to monitor:\n",
    "# 1. Training loss curve (should be smooth decrease)\n",
    "# 2. Validation loss vs training loss (check gap)\n",
    "# 3. Per-variable RMSE (check each variable converging)\n",
    "# 4. Learning rate schedule (verify cosine annealing)\n",
    "# 5. Training time per epoch (should be consistent)\n",
    "```\n",
    "\n",
    "This training infrastructure handles the complexity and scale of Norne while providing comprehensive monitoring and error handling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1c265",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
