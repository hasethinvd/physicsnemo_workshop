{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Physics-Attention vs Standard Attention\n",
        "\n",
        "**Goal:** Understand why standard transformers are expensive for physics simulations and how Physics-Attention solves this.\n",
        "\n",
        "## Outline\n",
        "1. Load Sample Stokes Flow Data\n",
        "2. The Quadratic Cost Problem\n",
        "3. Physics-Attention: The 4-Step Solution\n",
        "4. Visualize with Real Mesh Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from utils import softmax, download_stokes_dataset, load_stokes_sample\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Download & Load Stokes Flow Data\n",
        "\n",
        "We'll use the Stokes flow dataset (same as Lab 2). Run the cell below to download if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset (if not present) and load a sample\n",
        "download_stokes_dataset()\n",
        "coords, u, v, p = load_stokes_sample()\n",
        "N_mesh = len(coords)\n",
        "\n",
        "# Interactive visualization with Plotly\n",
        "fig = make_subplots(rows=1, cols=3, subplot_titles=['Velocity u', 'Velocity v', 'Pressure p'])\n",
        "\n",
        "fig.add_trace(go.Scatter(x=coords[:, 0], y=coords[:, 1], mode='markers',\n",
        "    marker=dict(size=5, color=u, colorscale='RdBu_r', showscale=True, \n",
        "                colorbar=dict(x=0.28, len=0.8, title='u')),\n",
        "    name='u', hovertemplate='x=%{x:.2f}<br>y=%{y:.2f}<br>u=%{marker.color:.3f}'), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=coords[:, 0], y=coords[:, 1], mode='markers',\n",
        "    marker=dict(size=5, color=v, colorscale='RdBu_r', showscale=True,\n",
        "                colorbar=dict(x=0.62, len=0.8, title='v')),\n",
        "    name='v', hovertemplate='x=%{x:.2f}<br>y=%{y:.2f}<br>v=%{marker.color:.3f}'), row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=coords[:, 0], y=coords[:, 1], mode='markers',\n",
        "    marker=dict(size=5, color=p, colorscale='Viridis', showscale=True,\n",
        "                colorbar=dict(x=0.97, len=0.8, title='p')),\n",
        "    name='p', hovertemplate='x=%{x:.2f}<br>y=%{y:.2f}<br>p=%{marker.color:.3f}'), row=1, col=3)\n",
        "\n",
        "fig.update_layout(title=f'Stokes Flow Data (N={N_mesh} mesh points)', height=400, width=1100, showlegend=False)\n",
        "fig.update_xaxes(title_text='x')\n",
        "fig.update_yaxes(title_text='y', scaleanchor='x', scaleratio=1)\n",
        "fig.show()\n",
        "\n",
        "print(f\"✓ Loaded mesh with {N_mesh} points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Standard Attention vs Physics-Attention\n",
        "\n",
        "### Standard Self-Attention (Transformers)\n",
        "\n",
        "Standard self-attention computes: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$\n",
        "\n",
        "**What are Q, K, V?**\n",
        "- **Q (Query)**: \"What information am I looking for?\" — Each point asks a question\n",
        "- **K (Key)**: \"What information do I have?\" — Each point advertises its content  \n",
        "- **V (Value)**: \"What information to send?\" — The actual content to aggregate\n",
        "\n",
        "In standard attention, **every point queries every other point**:\n",
        "```\n",
        "For N mesh points:\n",
        "  Q = X @ W_q  →  (N, d)   # N queries\n",
        "  K = X @ W_k  →  (N, d)   # N keys\n",
        "  V = X @ W_v  →  (N, d)   # N values\n",
        "  \n",
        "  Attention = softmax(Q @ K^T / √d) @ V\n",
        "                       ↑\n",
        "              This is N×N = O(N²) operations!\n",
        "```\n",
        "\n",
        "### Why This is Problematic for Simulations\n",
        "\n",
        "| Mesh Points (N) | Attention Matrix | Memory (float32) | Feasible? |\n",
        "|-----------------|------------------|------------------|-----------|\n",
        "| 1,000 | 1M ops | 4 MB | ✓ OK |\n",
        "| 10,000 | 100M ops | 400 MB | ⚠️ Slow |\n",
        "| 100,000 | 10B ops | 40 GB | ❌ Too expensive |\n",
        "\n",
        "Real CFD meshes often have 100K+ points — standard attention is infeasible!\n",
        "\n",
        "### Physics-Attention: The Key Difference\n",
        "\n",
        "Instead of every point attending to every other point, **group similar physics together**:\n",
        "\n",
        "```\n",
        "Standard:  Point → Point attention    (N×N)\n",
        "Physics:   Point → Slice → Point      (N×M + M×M + M×N)\n",
        "```\n",
        "\n",
        "**In Physics-Attention:**\n",
        "- **Q, K, V are computed on M slice tokens** (not N points!)\n",
        "- Slices are learned groupings of mesh points with similar physical behavior\n",
        "- Typical M = 8-64, while N = 10,000+\n",
        "\n",
        "This reduces complexity from **O(N²)** to **O(N·M + M²) ≈ O(N·M)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define both attention functions to compare\n",
        "N = N_mesh  # Use actual mesh size\n",
        "C = 8       # Feature dimension (e.g., coordinates + physics quantities)\n",
        "\n",
        "def standard_attention(X, d_k=8):\n",
        "    \"\"\"\n",
        "    Standard self-attention: O(N²) complexity\n",
        "    \n",
        "    X: (N, C) - features for N mesh points\n",
        "    Returns: output (N, C), attention matrix (N, N)\n",
        "    \"\"\"\n",
        "    # Q, K, V projections - each mesh point gets its own query/key/value\n",
        "    Q = X  # Query: (N, C) - \"What am I looking for?\" for each of N points\n",
        "    K = X  # Key:   (N, C) - \"What do I contain?\" for each of N points\n",
        "    V = X  # Value: (N, C) - \"What to send?\" for each of N points\n",
        "    \n",
        "    # THE EXPENSIVE PART: Every point attends to every other point\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (N, N) - N² dot products!\n",
        "    attn = softmax(scores, axis=1)   # (N, N) - attention weights\n",
        "    \n",
        "    output = attn @ V  # (N, C) - weighted combination of all values\n",
        "    return output, attn\n",
        "\n",
        "def physics_attention(X, M, d_k=8):\n",
        "    \"\"\"\n",
        "    Physics-Attention: O(N·M) complexity where M << N\n",
        "    \n",
        "    Key insight: Instead of N points having Q/K/V, we have M SLICES with Q/K/V!\n",
        "    \n",
        "    X: (N, C) - features for N mesh points\n",
        "    M: number of physics-based slices (typically 8-64)\n",
        "    Returns: output (N, C), slice weights (N, M), slice attention (M, M)\n",
        "    \"\"\"\n",
        "    N, C = X.shape\n",
        "    \n",
        "    # Step 1: SLICE - soft-assign N points to M slices\n",
        "    W_slice = np.random.randn(C, M) * 0.5  # Learnable projection\n",
        "    slice_weights = softmax(X @ W_slice, axis=1)  # (N, M) - which slice each point belongs to\n",
        "    \n",
        "    # Step 2: AGGREGATE - compress N points into M slice tokens\n",
        "    slice_tokens = slice_weights.T @ X  # (M, C) - weighted average per slice\n",
        "    \n",
        "    # Step 3: ATTEND - Q/K/V on SLICES, not points!\n",
        "    Q = slice_tokens  # (M, C) - only M queries (not N!)\n",
        "    K = slice_tokens  # (M, C) - only M keys\n",
        "    V = slice_tokens  # (M, C) - only M values\n",
        "    scores = Q @ K.T / np.sqrt(d_k)  # (M, M) - only M² ops, not N²!\n",
        "    slice_attn = softmax(scores, axis=1)  # (M, M)\n",
        "    attended = slice_attn @ V  # (M, C)\n",
        "    \n",
        "    # Step 4: DESLICE - broadcast M tokens back to N points\n",
        "    output = slice_weights @ attended  # (N, C)\n",
        "    \n",
        "    return output, slice_weights, slice_attn\n",
        "\n",
        "# Create sample input\n",
        "X = np.random.randn(N, C)\n",
        "\n",
        "# Compare costs for different M values\n",
        "print(\"=\"*70)\n",
        "print(f\"COST COMPARISON: Standard vs Physics-Attention (N = {N:,} mesh points)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{'Method':<25} {'Attention Size':<18} {'Operations':<15} {'Speedup':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Standard attention\n",
        "std_out, std_attn = standard_attention(X)\n",
        "std_ops = N * N\n",
        "print(f\"{'Standard Attention':<25} {f'{N}×{N}':<18} {std_ops:,} ops{'':<5} {'1x (baseline)':<10}\")\n",
        "\n",
        "# Physics attention with different M\n",
        "for M in [4, 8, 16, 32, 64]:\n",
        "    phys_out, slice_w, slice_attn = physics_attention(X, M)\n",
        "    # Total ops: N·M (slice) + M² (attend) + N·M (deslice) ≈ 2·N·M + M²\n",
        "    phys_ops = 2 * N * M + M * M\n",
        "    speedup = std_ops / phys_ops\n",
        "    print(f\"{'Physics-Attention M='+str(M):<25} {f'{M}×{M}':<18} {phys_ops:,} ops{'':<5} {speedup:.0f}x faster\")\n",
        "\n",
        "print(\"-\"*70)\n",
        "print(f\"\\n✓ With M=8 slices, we get ~{std_ops // (2*N*8 + 64):.0f}x speedup!\")\n",
        "print(\"✓ The key: M×M attention instead of N×N\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Physics-Attention: The 4-Step Solution\n",
        "\n",
        "### What are \"Slices\"?\n",
        "\n",
        "In physics simulations, different regions of a mesh often have similar physical behavior:\n",
        "- **Inlet region**: Smooth laminar flow\n",
        "- **Obstacle wake**: Turbulent/recirculating flow  \n",
        "- **Boundary layer**: High gradients near walls\n",
        "- **Far-field**: Nearly uniform flow\n",
        "\n",
        "**Slices** are learned groupings that cluster mesh points with similar physics. Instead of every point attending to every other point (N×N), we:\n",
        "1. Group points into M slices (soft assignment via learned weights)\n",
        "2. Compute attention only between slice representations (M×M)\n",
        "\n",
        "### The 4-Step Algorithm\n",
        "\n",
        "```\n",
        "Input: X ∈ R^(N×C)  (N mesh points, C features)\n",
        "       W ∈ R^(C×M)  (learnable slice weights)\n",
        "\n",
        "Step 1 - SLICE:     S = softmax(X @ W)           → S ∈ R^(N×M)  (assignment weights)\n",
        "Step 2 - AGGREGATE: Z = S^T @ X                  → Z ∈ R^(M×C)  (slice tokens)\n",
        "Step 3 - ATTEND:    Z' = Attention(Z, Z, Z)      → Z' ∈ R^(M×C) (M×M attention!)\n",
        "Step 4 - DESLICE:   Y = S @ Z'                   → Y ∈ R^(N×C)  (broadcast back)\n",
        "\n",
        "Output: Y ∈ R^(N×C)\n",
        "```\n",
        "\n",
        "**Key insight:** Step 3 is O(M²) instead of O(N²), and M is typically 8-64 while N can be 10,000+!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create physics-based features from our mesh (coords + physics values)\n",
        "C = 8  # Feature dimension\n",
        "features = np.column_stack([\n",
        "    coords,  # x, y coordinates\n",
        "    u.reshape(-1, 1),  # velocity u\n",
        "    v.reshape(-1, 1),  # velocity v  \n",
        "    p.reshape(-1, 1),  # pressure\n",
        "    np.random.randn(N, C-5)  # padding to get C features\n",
        "])\n",
        "\n",
        "print(f\"✓ Created feature matrix: {features.shape} (N={N} points, C={C} features)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. How Mesh Points Get Distributed to Slices\n",
        "\n",
        "Let's visualize exactly how mesh points get assigned to slices. Each point gets a **soft assignment weight** to each slice (values sum to 1). The point \"belongs\" most strongly to the slice with highest weight.\n",
        "\n",
        "### Step-by-Step with M=3 Slices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed visualization with M=3 slices\n",
        "M_demo = 3\n",
        "np.random.seed(123)  # For reproducibility\n",
        "\n",
        "# Step 1: Compute slice assignment weights\n",
        "W_demo = np.random.randn(C, M_demo) * 0.8\n",
        "slice_logits = features @ W_demo\n",
        "slice_weights = softmax(slice_logits, axis=1)  # Shape: (N, 3)\n",
        "dominant_slice = np.argmax(slice_weights, axis=1)\n",
        "\n",
        "# Count points per slice\n",
        "slice_colors = ['#e41a1c', '#377eb8', '#4daf4a']  # Red, Blue, Green\n",
        "slice_names = ['Slice 0', 'Slice 1', 'Slice 2']\n",
        "counts = [np.sum(dominant_slice == i) for i in range(M_demo)]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"SLICE ASSIGNMENT SUMMARY (M={M_demo} slices)\")\n",
        "print(\"=\"*60)\n",
        "for i, (name, count) in enumerate(zip(slice_names, counts)):\n",
        "    pct = 100 * count / N\n",
        "    print(f\"  {name}: {count:,} points ({pct:.1f}%)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================================\n",
        "# FIGURE 1: Each Slice Shown Separately (Clear Visualization)\n",
        "# ============================================================\n",
        "fig1, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "# Show each slice in its own subplot\n",
        "for i in range(M_demo):\n",
        "    ax = axes[i]\n",
        "    mask = dominant_slice == i\n",
        "    \n",
        "    # Plot all points faded\n",
        "    ax.scatter(coords[:, 0], coords[:, 1], c='lightgray', s=4, alpha=0.3)\n",
        "    # Highlight this slice's points\n",
        "    ax.scatter(coords[mask, 0], coords[mask, 1], c=slice_colors[i], s=10, alpha=0.9)\n",
        "    \n",
        "    ax.set_title(f'Slice {i}: {counts[i]:,} points ({100*counts[i]/N:.1f}%)', \n",
        "                fontsize=12, fontweight='bold', color=slice_colors[i])\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "# Final subplot: All slices combined\n",
        "ax = axes[3]\n",
        "for i in range(M_demo):\n",
        "    mask = dominant_slice == i\n",
        "    ax.scatter(coords[mask, 0], coords[mask, 1], \n",
        "              c=slice_colors[i], s=8, alpha=0.7, label=f'Slice {i} ({counts[i]:,} pts)')\n",
        "ax.set_title('All Slices Combined', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_aspect('equal')\n",
        "ax.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "plt.suptitle(f'Mesh Points Distributed Across {M_demo} Slices', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# FIGURE 2: Physics-Attention Matrix (M×M instead of N×N)\n",
        "# ============================================================\n",
        "fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Left: Points per slice bar chart\n",
        "bars = ax1.bar(range(M_demo), counts, color=slice_colors, edgecolor='black', linewidth=1.5)\n",
        "ax1.set_xlabel('Slice ID', fontsize=11)\n",
        "ax1.set_ylabel('Number of Points', fontsize=11)\n",
        "ax1.set_title('Points per Slice', fontsize=12, fontweight='bold')\n",
        "ax1.set_xticks(range(M_demo))\n",
        "for bar, count in zip(bars, counts):\n",
        "    ax1.annotate(f'{count:,}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
        "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Right: The resulting M×M attention matrix\n",
        "phys_attn = softmax(np.random.randn(M_demo, M_demo), axis=1)\n",
        "im_attn = ax2.imshow(phys_attn, cmap='Greens', vmin=0, vmax=1)\n",
        "ax2.set_title(f'Physics-Attention: {M_demo}×{M_demo} = {M_demo**2} ops\\n(instead of {N}×{N} = {N*N:,} ops)', \n",
        "             fontsize=11, fontweight='bold')\n",
        "ax2.set_xlabel('Key Slice')\n",
        "ax2.set_ylabel('Query Slice')\n",
        "ax2.set_xticks(range(M_demo))\n",
        "ax2.set_yticks(range(M_demo))\n",
        "plt.colorbar(im_attn, ax=ax2, shrink=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✓ Cost reduction: {N*N:,} → {M_demo**2} = {N*N // M_demo**2:,}x fewer operations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effect of Different Slice Counts (M)\n",
        "\n",
        "More slices = finer grouping but higher cost. Typical values: M=8 to M=64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare M=4, M=8, M=16 slice partitioning\n",
        "slice_configs = [4, 8, 16]\n",
        "fig_compare, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, num_slices in enumerate(slice_configs):\n",
        "    ax = axes[idx]\n",
        "    np.random.seed(42 + idx)  # Different seed for variety\n",
        "    \n",
        "    # Compute slice assignments\n",
        "    W = np.random.randn(C, num_slices) * 0.6\n",
        "    logits = features @ W\n",
        "    weights = softmax(logits, axis=1)\n",
        "    dominant_slice = np.argmax(weights, axis=1)\n",
        "    \n",
        "    # Plot mesh colored by slice\n",
        "    scatter = ax.scatter(coords[:, 0], coords[:, 1], c=dominant_slice, \n",
        "                        cmap='tab10' if num_slices <= 10 else 'tab20',\n",
        "                        s=6, alpha=0.7)\n",
        "    \n",
        "    # Calculate cost reduction\n",
        "    cost_reduction = N*N // (num_slices**2)\n",
        "    ax.set_title(f'M = {num_slices} slices\\nAttention: {num_slices}×{num_slices}={num_slices**2} ops\\n({cost_reduction:,}x cheaper)', \n",
        "                fontsize=10)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_aspect('equal')\n",
        "    plt.colorbar(scatter, ax=ax, label='Slice ID', shrink=0.8)\n",
        "\n",
        "plt.suptitle('Trade-off: More Slices = Finer Resolution but Higher Cost', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTypical M values in Transolver: 8-64 (paper uses M=32 or M=64)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Aspect | Standard Attention | Physics-Attention |\n",
        "|--------|-------------------|-------------------|\n",
        "| **Complexity** | O(N²) — expensive! | O(N·M) — efficient! |\n",
        "| **Attention matrix** | N×N | M×M (M≈64) |\n",
        "| **Grouping** | All-to-all | Learned slices |\n",
        "\n",
        "**Key Takeaway:** Physics-Attention reduces cost by grouping mesh points into M learned \"slices\" and performing attention between these compressed representations.\n",
        "\n",
        "**Next:** Notebook 2 shows the full Transolver architecture in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
